{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d0f6845-e53f-4e9b-be24-0e376269f522",
   "metadata": {},
   "source": [
    "# Address Matching Deep Dive\n",
    "\n",
    "This notebook experiments with different ways of comparing addresses in order to demonstrate the power of parsed address comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feab8c8-a116-40f9-a882-5a932140af26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from numbers import Number\n",
    "from typing import Callable, Dict, List, Literal, Sequence, Tuple, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "import pytest\n",
    "import random\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from datasets import Dataset\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "from postal.parser import parse_address\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_curve,\n",
    "    precision_recall_fscore_support,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    auc\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers import InputExample, SentenceTransformer, SentencesDataset, SentenceTransformerTrainer, losses\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, SimilarityFunction, BinaryClassificationEvaluator\n",
    "from sentence_transformers.model_card import SentenceTransformerModelCardData\n",
    "from sentence_transformers.training_args import BatchSamplers, SentenceTransformerTrainingArguments\n",
    "from tenacity import retry\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.autonotebook import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel, EarlyStoppingCallback, TrainingArguments, Trainer\n",
    "from transformers.integrations import WandbCallback\n",
    "\n",
    "from utils import (\n",
    "    augment_gold_labels,\n",
    "    compute_sbert_metrics,\n",
    "    compute_classifier_metrics,\n",
    "    format_dataset,\n",
    "    gold_label_report,\n",
    "    preprocess_logits_for_metrics,\n",
    "    structured_encode_address,\n",
    "    tokenize_function,\n",
    "    to_dict,\n",
    "    save_custom_model,\n",
    "    load_custom_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0578d26d-790c-4fe6-aed7-2529f155f7f6",
   "metadata": {},
   "source": [
    "#### Pin Random Seeds for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b495c03-2aba-4d6b-889b-3b21b56f4d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 31337\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.mps.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e998da-0660-4274-b000-619b28c3c533",
   "metadata": {},
   "source": [
    "#### Setup Basic Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e508b00c-0011-4845-a4d6-cd131ec5b6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stderr, level=logging.ERROR)\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2912a2be-9e56-4bf9-94fc-3dbcfd02ef17",
   "metadata": {},
   "source": [
    "#### Ignore Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cd4a2c-f1f5-4c82-8c15-a52dc87db947",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore', FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ba8945-a02c-4661-a865-c03ee6eecebe",
   "metadata": {},
   "source": [
    "#### Configure Weights & Biases\n",
    "\n",
    "`wandb` needs some environment variables to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9491984a-c9fb-40d5-8fb0-743b77d15c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_LOG_MODEL\"] = \"end\"\n",
    "os.environ[\"WANDB_WATCH\"] = \"all\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"libpostal-reborn\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"false\"\n",
    "os.environ[\"WANDB_IGNORE_GLOBS\"] = \".env\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e1d411-e2a5-4824-9309-ff50861a0e19",
   "metadata": {},
   "source": [
    "#### Configure Huggingface APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d0d44b-c38a-48ca-a3b1-895b5b8c320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_ENDPOINT\"] = \"https://huggingface.co/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7462e0-b3d1-4d76-bd7a-803cbd2f3446",
   "metadata": {},
   "source": [
    "#### Configure Huggingface APIs\n",
    "\n",
    "Squash any warnings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcde550-79a6-4333-967e-c80d555de5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42913e46-e5db-4eff-9410-5a8449d4cbf6",
   "metadata": {},
   "source": [
    "#### Configure Pandas to Show More Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d4cb15-b201-4f34-8917-d292dd2a1876",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 40)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad42226-1abc-4c29-b68d-659d330ba1f9",
   "metadata": {},
   "source": [
    "### Use CUDA or MPS if Avaialable\n",
    "\n",
    "CPU training and even inference with sentence transformers and deep learning models is quite slow. Since all machine learning in this library is based on [PyTorch](https://pytorch.org/get-started/locally/), we can assign all ML operations to a GPU in this one block of code. Otherwise we default to CPU without acceleration. The notebook is still workable in this mode, you just may need to grab a cup of tea or coffee while you wait for it to train the Sentence-BERT model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1051aa37-26b7-496a-8182-f10cfa1f5f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for CUDA or MPS availability and set the device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    logger.debug(\"Using Apple GPU acceleration\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    logger.debug(\"Using NVIDIA CUDA GPU acceleration\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    logger.debug(\"Using CPU for ML\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921a41eb-4e6b-4d64-a3b4-46013524e37c",
   "metadata": {},
   "source": [
    "### Use Weights & Biases for Logging Metrics\n",
    "\n",
    "Weights & Biases has a free account for individuals with public projects. Using it will produce charts during our training runs that anyone can view. You can create your own project for this notebook and login with that key to log your own training runs.\n",
    "\n",
    "You may need to run the following command from your shell before the next cell, otherwise you will have to paste your project key into the \n",
    "\n",
    "```bash\n",
    "wandb login\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98cdf2f-fcb9-495e-9201-80f9e07d9070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to wandb. Comment out if you already haven't via `wandb login` from a CLI\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85701b19-a5e1-42c4-9ace-8d88d96f893f",
   "metadata": {},
   "source": [
    "## Implementing a Structured Address Matcher\n",
    "\n",
    "Let's start our exercise by using the structured address data provided by [Libpostal](https://github.com/openvenues/libpostal) to parse them for matching. We write a function for each address part to deal with missing fields without duplicating a lot of logic.\n",
    "\n",
    "We start with something quite literal and basic. We'll improve it as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44031098-15b5-45ef-b1d0-8a51a9838bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_match_address(address1: str, address2: str) -> Literal[0, 1]:\n",
    "    \"\"\"parse_match_address implements address matching using the precise, parsed structure of addresses.\"\"\"\n",
    "    address1 = to_dict(parse_address(address1))\n",
    "    address2 = to_dict(parse_address(address2))\n",
    "\n",
    "    def match_road(address1: Dict, address2: Dict) -> Literal[0, 1]:\n",
    "        \"\"\"match_road - literal road matching, negative if either lacks a road\"\"\"\n",
    "        if (\"road\" in address1) and (\"road\" in address2):\n",
    "            if address1[\"road\"] == address2[\"road\"]:\n",
    "                logger.debug(\"road match\")\n",
    "                return 1\n",
    "            else:\n",
    "                logger.debug(\"road mismatch\")\n",
    "                return 0\n",
    "        logger.debug(\"road mismatch\")\n",
    "        return 0\n",
    "\n",
    "    def match_house_number(address1: Dict, address2: Dict) -> Literal[0, 1]:\n",
    "        \"\"\"match_house_number - literal house number matching, negative if either lacks a house_number\"\"\"\n",
    "        if (\"house_number\" in address1) and (\"house_number\" in address2):\n",
    "            if address1[\"house_number\"] == address2[\"house_number\"]:\n",
    "                logger.debug(\"house_number match\")\n",
    "                return 1\n",
    "            else:\n",
    "                logger.debug(\"house_number mismatch\")\n",
    "                return 0\n",
    "        logger.debug(\"house_number mistmatch\")\n",
    "        return 0\n",
    "\n",
    "    def match_unit(address1: Dict, address2: Dict) -> Literal[0, 1]:\n",
    "        \"\"\"match_unit - note a missing unit in both is a match\"\"\"\n",
    "        if \"unit\" in address1:\n",
    "            if \"unit\" in address2:\n",
    "                logger.debug(\"unit match\")\n",
    "                return 1 if (address1[\"unit\"] == address2[\"unit\"]) else 0\n",
    "            else:\n",
    "                logger.debug(\"unit mismatch\")\n",
    "                return 0\n",
    "        if \"unit\" in address2:\n",
    "            if \"unit\" in address1:\n",
    "                logger.debug(\"unit match\")\n",
    "                return 1 if (address1[\"unit\"] == address2[\"unit\"]) else 0\n",
    "            else:\n",
    "                logger.debug(\"unit mismatch\")\n",
    "                return 0\n",
    "        # Neither address has a unit, which is a default match\n",
    "        return 1\n",
    "\n",
    "    def match_postcode(address1: Dict, address2: Dict) -> Literal[0, 1]:\n",
    "        \"\"\"match_postcode - literal matching, negative if either lacks a postal code\"\"\"\n",
    "        if (\"postcode\" in address1) and (\"postcode\" in address2):\n",
    "            if address1[\"postcode\"] == address2[\"postcode\"]:\n",
    "                logger.debug(\"postcode match\")\n",
    "                return 1\n",
    "            else:\n",
    "                logger.debug(\"postcode mismatch\")\n",
    "                return 0\n",
    "        logger.debug(\"postcode mismatch\")\n",
    "        return 0\n",
    "\n",
    "    def match_country(address1: Dict, address2: Dict) -> Literal[0, 1]:\n",
    "        \"\"\"match_country - literal country matching - pass if both don't have one\"\"\"\n",
    "        if (\"country\" in address1) and (\"country\" in address2):\n",
    "            if address1[\"country\"] == address2[\"country\"]:\n",
    "                logger.debug(\"country match\")\n",
    "                return 1\n",
    "            else:\n",
    "                logger.debug(\"country mismatch\")\n",
    "                return 0\n",
    "        # One or none countries should match\n",
    "        logger.debug(\"country match\")\n",
    "        return 1\n",
    "\n",
    "    # Combine the above to get a complete address matcher\n",
    "    if (\n",
    "        match_road(address1, address2)\n",
    "        and match_house_number(address1, address2)\n",
    "        and match_unit(address1, address2)\n",
    "        and match_postcode(address1, address2)\n",
    "        and match_country(address1, address2)\n",
    "    ):\n",
    "        logger.debug(\"overall match\")\n",
    "        return 1\n",
    "    else:\n",
    "        logger.debug(\"overall mismatch\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f67113-6f5f-45cb-9f4c-4ee0a4362de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yup down to house_number ...\n",
    "parse_match_address(\n",
    "    \"3413 Sean Way, Lawrenceville, GA 30044\",\n",
    "    \"3413 Sean Way Lawrenceville, GA, 30044\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea361bd-56df-4980-a060-1d7e743af38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz.ratio(\n",
    "    \"3413 Sean Way, Lawrenceville, GA 30044\",\n",
    "    \"3413 Sean Way Lawrenceville, GA, 30044\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797ace98-a912-4c45-b178-6c6201f58bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yup down to unit ...\n",
    "parse_match_address(\n",
    "    \"120 Ralph McGill Blvd, Apt 101, Atlanta, GA 30308, USA\",\n",
    "    \"120 Ralph McGill Blvd, Apt 101, Atlanta GA 30308\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caf28fb-64e1-447f-a996-6d0d9e67bb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz.ratio(\n",
    "    \"120 Ralph McGill Blvd, Apt 101, Atlanta, GA 30308, USA\",\n",
    "    \"120 Ralph McGill Blvd, Apt 101, Atlanta GA 30308\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b949ebe-8889-498c-a601-904c2c5c1433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nope if only one uses an abbreviation ...\n",
    "parse_match_address(\n",
    "    \"120 Ralph McGill Blvd, Apt 333, Atlanta, GA 30308\",\n",
    "    \"120 Ralph McGill Boulevard, Apt 333, Atlanta, GA 30308\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108b88c9-9aca-4a13-80f9-2c88dbe77afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz.ratio(\n",
    "    \"120 Ralph McGill Blvd, Apt 333, Atlanta, GA 30308\",\n",
    "    \"120 Ralph McGill Boulevard, Apt 333, Atlanta, GA 30308\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498ab13c-1457-4c4c-95ff-2f691f697738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nope if one character in the streetname is off ...\n",
    "parse_match_address(\n",
    "    \"120 Ralp McGill Blvd, Apt 333, Atlanta, GA 30308\",\n",
    "    \"120 Ralph McGill Blvd, Apt 333, Atlanta, GA 30308\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6681fa67-bdf4-43b0-984d-8c91e5cbf902",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz.ratio(\n",
    "    \"120 Ralp McGill Blvd, Apt 333, Atlanta, GA 30308\",\n",
    "    \"120 Ralph McGill Blvd, Apt 333, Atlanta, GA 30308\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f1a4c5-0bcd-4c52-b5d7-7be7ad1932ea",
   "metadata": {},
   "source": [
    "### Literal is Too Precise!\n",
    "\n",
    "While it is useful to parse addresses and implement literal matching logic as we did above, as the third example indicates, an abbreviation or a single typo results in a mistmatch. We're going to write a more complex, approximate logical matcher below using string distance and text embeddings.\n",
    "\n",
    "Depending on your application you might relax this criteria to include corner cases such as missing postcodes. Before we get into that, let's create some training and evaluation data using hand-labeled data with an LLM data augmentation strategy to generate a lot of labeled records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15895b85-1d0f-441c-9d20-0849b3db3b22",
   "metadata": {},
   "source": [
    "## Data Augmentation with the OpenAI GPT4o API\n",
    "\n",
    "We need training data for our supervised learning approaches to addres matching. Open the sister notebook [Address Data Augmentation.ipynb](Address%20Matching%20Deep%20Dive.ipynb) before procceeding to further cells in order to create some training data via minimal manual labeling and programmatic data labeling for data augmentation. This will teach you programmatic data labeling, a critical skill that LLMs make MUCH EASIER because they understand things like the semantics of global addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833e8045-5cd3-4b5a-9cd3-afc65965062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df = pd.read_csv(\"data/gold.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6382fc3-ca84-4d17-9d74-354ab798d21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to start from here and not run the data augmentation pipeline again...\n",
    "augment_results_df = pd.read_parquet(\"data/training.6.parquet\")\n",
    "\n",
    "augment_results_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2b2e4e-0c28-47ce-8b3c-7c1dac0062f5",
   "metadata": {},
   "source": [
    "### Data Augmentation Complete!\n",
    "\n",
    "Starting by hand labeling under 100 records and iterating a few times on data augmentation instructions for GPT4o, we have multiplied them by many times to get almost 10,000 synthetic records! This is enough to fine-tune a `SentenceTransformer` or semantic text similarity classifier model. GPT4o is a powerful tool for data augmentation! This can work for a variety of problems.\n",
    "\n",
    "LLM based data augmentation is a powerful tool for your data labeling toolbox."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423992c9-bd0c-4e85-b5ce-cbf14a1d5356",
   "metadata": {},
   "source": [
    "# Comparing Different Approaches to Address Matching\n",
    "\n",
    "Now we're going to compare the following methods of address matching:\n",
    "\n",
    "1) Database Lookups - we'l use [pycountry](https://pypi.org/project/pycountry/) ([github](https://github.com/pycountry/pycountry)) to improve international address matching (see [PyCountry Nation Matching](PyCountry%20Nation%20Matching.ipynb)).\n",
    "2) Text Embeddings - we'll use transfer learning to load an existing [SentenceTransformer](https://sbert.net) model to sentence encode pairs of addresses to create fixed-length embeddings for each address and then compute a similarity score via [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity). This won't work without fine-tuning, so we fine-tune the model to the task.\n",
    "3) Deep Matching Model - We'll train a deep semantic textual similarity classification model based on a Siamese BERT network as defined in [Sentence-BERT](https://arxiv.org/abs/1908.10084) to classify address pairs as matching or not matching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7aa526-33e4-4705-a453-585ab7f3869a",
   "metadata": {},
   "source": [
    "# Machine Learning Approaches to Address Matching\n",
    "\n",
    "In this section we pursue two machine learning approaches to address matching, in order of sophistication. First we fine-tune a pre-trained embedding model to our task, try it on our data and search for a threshold similarity that results in good performance for our address matching problem. Second we build a Siamese BERT network model based on [Sentence-BERT](https://arxiv.org/abs/1908.10084) to classify pairs of addresses as match or mismatch. We will train it using the same dataset we use to fine-tune a sentence transformer, and if we have enough training data this will likely be a more powerful approach.\n",
    "\n",
    "## Text Embeddings, Sentence Encoding, `SentenceTransformers`, Vector Distance and Cosine Similarity\n",
    "\n",
    "Text embeddings are trained on large volumes of text that include addresses. As a result they have some understanding of address strings and can do a form of semantic comparison that is less explicit than logical comparisons with address parsing. They're an important benchmark to explore. Huggingface has an excellent [introduction to sentence similarity](https://huggingface.co/tasks/sentence-similarity).\n",
    "\n",
    "In our first machine learning approach, we are going to use transfer learning to load a pre-trained [sentence transformer](https://sbert.net) models from huggingface. We will use the training data we've prepared to fine-tune this model to our task, before rigorously evaluating it along with our other approaches.\n",
    "\n",
    "Sentence transformers sentence encode strings of different distances into fixed-length vectors, a technique called sentence encoding. Once two address strings are embedded into a pair of equal length vectors, they can be compared with cosine similarity to get a distance, the inverse of which is a similarity score.\n",
    "\n",
    "### Convert our `pd.DataFrame` to a `List[sentence_transformers.InputExample]`\n",
    "\n",
    "First we need to convert our Pandas `DataFrame` to a list of sentence transformer input examples. `InputExamples` require two fields `texts=List[str, str]` and `label`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b105924-4872-4abf-a10c-4d77150f1b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, tmp_df = train_test_split(augment_results_df, test_size=0.2, shuffle=True)\n",
    "eval_df, test_df = train_test_split(tmp_df, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"sentence1\": train_df[\"Address1\"].tolist(),\n",
    "    \"sentence2\": train_df[\"Address2\"].tolist(),\n",
    "    \"label\": train_df[\"Label\"].tolist(),\n",
    "})\n",
    "\n",
    "eval_dataset = Dataset.from_dict({\n",
    "    \"sentence1\": eval_df[\"Address1\"].tolist(),\n",
    "    \"sentence2\": eval_df[\"Address2\"].tolist(),\n",
    "    \"label\": eval_df[\"Label\"].tolist(),\n",
    "})\n",
    "\n",
    "test_dataset = Dataset.from_dict({\n",
    "    \"sentence1\": test_df[\"Address1\"].tolist(),\n",
    "    \"sentence2\": test_df[\"Address2\"].tolist(),\n",
    "    \"label\": test_df[\"Label\"].tolist(),\n",
    "})\n",
    "\n",
    "print(f\"Training data:   {len(train_df):,}\")\n",
    "print(f\"Validation data: {len(eval_df):,}\")\n",
    "print(f\"Test data        {len(eval_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680e51e3-84d0-40e5-97d1-ebce61cd9514",
   "metadata": {},
   "source": [
    "### Configure Fine-Tuning, Initialize a `SentenceTransformer`\n",
    "\n",
    "To use the training data we prepared to fine-tune a `SentenceTransformer`, we need to select and load a pre-trained model from Huggingface Hub. Here are some models you can try:\n",
    "\n",
    "* [sentence-transformers/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) - multilingual paraphrase models are designed to compare sentences in terms of their semantics.\n",
    "* [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3) - a robust, multilingual model optimized for a variety of tasks\n",
    "* [sentence-transformers/paraphrase-multilingual-mpnet-base-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2) - MPNet is another paraphrase model architecture we can fine-tune for address comparison\n",
    "* [sentence-transformers/all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) - a top performing MPNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a2a181-a9cd-469f-9b1b-32b9d3163dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SBERT_MODEL = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "VARIANT = \"original\"\n",
    "MODEL_SAVE_NAME = (SBERT_MODEL + \"-\" + VARIANT).replace(\"/\", \"-\")\n",
    "\n",
    "# Make sure these match the values in the data augmentation notebook for accurate loggging and reporting\n",
    "CLONES_PER_RUN = 100\n",
    "RUNS_PER_EXAMPLE = 2\n",
    "\n",
    "EPOCHS = 6\n",
    "BATCH_SIZE = 32\n",
    "PATIENCE = 2\n",
    "LEARNING_RATE = .00005\n",
    "DATASET_MULTIPLE = CLONES_PER_RUN * RUNS_PER_EXAMPLE\n",
    "SBERT_OUTPUT_FOLDER = f\"data/fine-tuned-sbert-{MODEL_SAVE_NAME}\"\n",
    "SAVE_EVAL_STEPS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e174e134-4761-4732-a2f7-98a5a785155c",
   "metadata": {},
   "source": [
    "### Initialize Weights & Biases\n",
    "\n",
    "Weights and biases `wandb` package makes it simple to monitor the performance of your training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68f7a53-d09a-4e8b-b5e9-f62857c12e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weights & Biases\n",
    "wandb.init(\n",
    "    entity=\"rjurney\",\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"libpostal-reborn\",\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"variant\": VARIANT,\n",
    "        \"dataset_multiple\": DATASET_MULTIPLE,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"patience\": PATIENCE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"sbert_model\": SBERT_MODEL,\n",
    "        \"model_save_name\": MODEL_SAVE_NAME,\n",
    "        \"sbert_output_folder\": SBERT_OUTPUT_FOLDER,\n",
    "        \"save_eval_steps\": SAVE_EVAL_STEPS,\n",
    "    },\n",
    "    save_code=True,\n",
    "    save_checkpoint=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0f82a6-1fb7-45fe-b468-9df611413f1e",
   "metadata": {},
   "source": [
    "### Setup our `SentenceTransformer` Model\n",
    "\n",
    "Choose the model to fine-tune above in `SBERT_MODEL` and instantiate it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e40fb24-77a6-4291-b0c5-076f8a9997d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = SentenceTransformer(\n",
    "    SBERT_MODEL,\n",
    "    device=device,\n",
    "    model_card_data=SentenceTransformerModelCardData(\n",
    "        language=\"en\",\n",
    "        license=\"apache-2.0\",\n",
    "        model_name=f\"{SBERT_MODEL}-address-matcher-{VARIANT}\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7285812b-dde6-4af0-b0d0-f3ed7c777a51",
   "metadata": {},
   "source": [
    "### Evaluate our Model Before Fine-Tuning\n",
    "\n",
    "Let's see what it can do without fine-tuning, then we'll compare our subjective results afterwards. This won't work very well, fine-tuning is required!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd45043-cac4-47d1-af34-c80fbac09e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sbert_compare(address1: str, address2: str) -> float:\n",
    "    \"\"\"sbert_compare - sentence encode each address into a fixed-length text embedding.\n",
    "    Fixed-length means they can be compared with cosine similarity.\"\"\"\n",
    "    embedding1 = sbert_model.encode(address1)\n",
    "    embedding2 = sbert_model.encode(address2)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    return 1 - distance.cosine(embedding1, embedding2)\n",
    "\n",
    "\n",
    "def sbert_match(row: pd.Series) -> pd.Series:\n",
    "    \"\"\"sbert_match - SentenceTransformer address matching, float iytoyt\"\"\"\n",
    "    return sbert_compare(row[\"Address1\"], row[\"Address2\"])\n",
    "\n",
    "\n",
    "def sbert_compare_binary(address1: str, address2: str, threshold: float = 0.5) -> Literal[0, 1]:\n",
    "    \"\"\"sbert_match - compare and return a binary match\"\"\"\n",
    "    similarity = sbert_compare(address1, address2)\n",
    "    return 1 if similarity >= threshold else 0\n",
    "\n",
    "\n",
    "def sbert_match_binary(row: pd.Series, threshold: float = 0.5) -> pd.Series:\n",
    "    \"\"\"sbert_match_binary - SentenceTransformer address matching, binary output\"\"\"\n",
    "    return sbert_compare_binary(row[\"Address1\"], row[\"Address2\"], threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b083ad79-6b50-414f-83de-a3ae68f44c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still too similar - very hard to train them away from this behavior!\n",
    "sbert_compare(\n",
    "    \"101 Oak Lane, Atlanta, GA 30308\",\n",
    "    \"102 Oak Lane, Atlanta, GA 30308\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65dad9f-3f08-439d-8ee8-2b14b740a55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A little bit further away ...\n",
    "sbert_compare(\n",
    "    \"101 Oak Lane, Atlanta, GA 30308\",\n",
    "    \"101 Oak Ln., Atlanta, GA 30308\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f869c21a-8b41-4520-87bd-7dbc8d37d1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Properly distant ...\n",
    "sbert_compare(\n",
    "    \"3413 Sean Way, Lawrenceville, GA 30044\",\n",
    "    \"1202 Oak Rd., Lawrenceville, GA 30304\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aba7fb-a560-4869-b05d-414a03a34542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Properly similar ...\n",
    "sbert_compare(\n",
    "    \"3413 Sean Way, Lawrenceville, GA 30044\",\n",
    "    \"3413 Sean Way, Lawrenceville, GA 30044, USA\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a0a9d8-8c06-45dd-9ff4-7b056e078e5a",
   "metadata": {},
   "source": [
    "### Evaluate the Test Set with the Untrained Model\n",
    "\n",
    "Let's see how well the [paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) model does on its own. This is our baseline score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e9a873-9044-443b-9741-3ab3fed0a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the evaluator\n",
    "binary_acc_evaluator = BinaryClassificationEvaluator(\n",
    "    sentences1=eval_dataset[\"sentence1\"],\n",
    "    sentences2=eval_dataset[\"sentence2\"],\n",
    "    labels=eval_dataset[\"label\"],\n",
    "    name=SBERT_MODEL,\n",
    ")\n",
    "pd.DataFrame([binary_acc_evaluator(sbert_model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d22615-249a-4ee7-b7c5-43fb1cada980",
   "metadata": {},
   "source": [
    "### Computing Metrics with `sklearn.metrics`\n",
    "\n",
    "We use [scikit-learn metrics](https://scikit-learn.org/stable/modules/model_evaluation.html) instead to compute our evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31111f8e-0167-4a6b-abff-dc3ecf391687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will rapidly train the embedding model. MultipleNegativesRankingLoss did not work.\n",
    "loss = losses.ContrastiveLoss(model=sbert_model)\n",
    "\n",
    "sbert_args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=SBERT_OUTPUT_FOLDER,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    warmup_ratio=0.1,\n",
    "    run_name=SBERT_MODEL,\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=5,\n",
    "    save_steps=SAVE_EVAL_STEPS,\n",
    "    eval_steps=SAVE_EVAL_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    greater_is_better=False,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=sbert_model,\n",
    "    args=sbert_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    loss=loss,\n",
    "    evaluator=binary_acc_evaluator,\n",
    "    compute_metrics=compute_sbert_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=PATIENCE)],\n",
    ")\n",
    "\n",
    "trainer.evaluate()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fd31d5-005d-4338-973e-663eb7589443",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best model checkpoint path: {trainer.state.best_model_checkpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0340162d-e84f-496b-bc8c-f730d327205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([trainer.evaluate()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e575f148-b116-4dd3-b9fb-4c99d3027778",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(SBERT_OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f52923-5fc9-4e0c-8e5d-d2cb21d68246",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ad1454-2902-483b-9500-63456cfa5e8d",
   "metadata": {},
   "source": [
    "### Try the Model from Our Best Epoch\n",
    "\n",
    "We fine-tuned the model for `EPOCHS` nubmer of epochs, but the last epoch isn't always best. The `TrainingArgument` `load_best_model_at_end=True` loads the model at the end.\n",
    "\n",
    "Another way to load the best model is to load our output folder and evaluate that `SentenceTransformer` on some examples to get a gestalt sense for its performance.\n",
    "\n",
    "```python\n",
    "sbert_model = SentenceTransformer(OUTPUT_FOLDER, device=device)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f2c9c5-5f70-4f13-8b3a-8e24d8509872",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_compare(\n",
    "    \"101 Oak Lane, Atlanta, GA 30308\",\n",
    "    \"102 Oak Lane, Atlanta, GA 30308\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246f35ba-5e5f-4801-a5dd-e0b54bc34d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_compare(\n",
    "    \"101 Oak Lane, Macon, GA 30308\",\n",
    "    \"101 Oak Lane, Atlanta, GA 30408\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96ef967-0c99-4301-9c2c-6909360566e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_compare(\n",
    "    \"101 Oak Lane, Atlanta, GA 30308\",\n",
    "    \"101 Oak Ln., Atlanta, GA 30308\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eb3771-17c6-4d85-9bad-8615c1eb8db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sbert_compare(\n",
    "    \"3413 Sean Way, Lawrenceville, GA 30044\",\n",
    "    \"1202 Oak Rd., Lawrenceville, GA 30304\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c78fd25-be01-4052-b717-6471730d92ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_compare(\n",
    "    \"3413 Sean Way, Lawrenceville, GA 30044\",\n",
    "    \"3413 Sean Way, Lawrenceville, GA 30044, USA\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441a2b8b-2525-4c7c-98c0-6a65f69661b0",
   "metadata": {},
   "source": [
    "### Evaluate ROC Curve to Determine Optimum Similarity Threshold\n",
    "\n",
    "0.5 is an arbitrary line on which to divide positive (match, 1) and negative (mismatch, 0). Let's evaluate the ROC Curve of the F1 score to see what it should be set to. Recall that the `sbert_match` function has a `threshold: float = 0.5` argument.\n",
    "\n",
    "#### Evaluate on our Augmented Test Dataset\n",
    "\n",
    "First we'll evaluate the ROC curve on our augmented test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53242e2-7ce0-4c00-853f-277ba98b6317",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_df[\"Label\"]\n",
    "y_scores = test_df.apply(sbert_match, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb6dc8-528b-4680-9794-5da506b02502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "\n",
    "# Compute F1 score for each threshold\n",
    "f1_scores = [f1_score(y_true, y_scores >= t) for t in thresholds]\n",
    "\n",
    "# Find the threshold that maximizes the F1 score\n",
    "best_threshold_index = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_threshold_index]\n",
    "best_f1_score = f1_scores[best_threshold_index]\n",
    "\n",
    "print(f'Best Threshold: {best_threshold}')\n",
    "print(f'Best F1 Score: {best_f1_score}')\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_scores)\n",
    "print(f'AUC-ROC: {roc_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737566a7-4902-468f-b87e-585650d484fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for Seaborn\n",
    "pr_data = pd.DataFrame({\n",
    "    'Precision': precision[:-1],\n",
    "    'Recall': recall[:-1],\n",
    "    'F1 Score': f1_scores\n",
    "})\n",
    "\n",
    "# Plot Precision-Recall curve using Seaborn\n",
    "sns.lineplot(data=pr_data, x='Recall', y='Precision', marker='o')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Augmented Test Set Precision-Recall Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f50818-d35c-43b9-b022-9746ce96cdc2",
   "metadata": {},
   "source": [
    "### Plot a ROC Curve for our Gold Labeled Data\n",
    "\n",
    "We need to see the ROC Curve for our gold labeled data as well. We care more about performance on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3eae08-beef-419a-bb8a-ffe74fae4cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = gold_df[\"Label\"]\n",
    "y_scores = gold_df.apply(sbert_match, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bfde9f-9610-4b47-88d6-4432d5081f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "\n",
    "# Compute F1 score for each threshold\n",
    "f1_scores = [f1_score(y_true, y_scores >= t) for t in thresholds]\n",
    "\n",
    "# Find the threshold that maximizes the F1 score\n",
    "best_threshold_index = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_threshold_index]\n",
    "best_f1_score = f1_scores[best_threshold_index]\n",
    "\n",
    "print(f'Best Threshold: {best_threshold}')\n",
    "print(f'Best F1 Score: {best_f1_score}')\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_scores)\n",
    "print(f'AUC-ROC: {roc_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe142c8-8b4b-4384-be50-4e77413fd40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for Seaborn\n",
    "pr_data = pd.DataFrame({\n",
    "    'Precision': precision[:-1],\n",
    "    'Recall': recall[:-1],\n",
    "    'F1 Score': f1_scores\n",
    "})\n",
    "\n",
    "# Plot Precision-Recall curve using Seaborn\n",
    "sns.lineplot(data=pr_data, x='Recall', y='Precision', marker='o')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Gold Label Precision-Recall Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768d6185-8345-4460-82fc-654fd63cec04",
   "metadata": {},
   "source": [
    "### Debugging Errors on our Gold Labels\n",
    "\n",
    "Let's evaluate the data using our `gold_label_report` function with the best F1 score. Then we can view the errors and figure out where our model is failing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b16f14-ee29-4e2b-acec-61df4bfe8add",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df, grouped_df = gold_label_report(\n",
    "    gold_df,\n",
    "    [\n",
    "        # strict_parse_match,\n",
    "        # parse_match_country,\n",
    "        sbert_match_binary,\n",
    "    ],\n",
    "    threshold=best_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd151a3-f774-45ce-857f-e3d072fc803d",
   "metadata": {},
   "source": [
    "#### Label Description Group Analysis\n",
    "\n",
    "You can see the types of address pairs we are failing on. This can guide our data augmentation / programmatic labeling work at a high level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c954ca92-334d-4d73-98a2-313bbec12c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edae3d6-4085-41d9-bacb-3efde3d48e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df[\"sbert_match_binary_acc\"].sort_values().head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4340fd-af40-4450-bfc1-834bac0851cb",
   "metadata": {},
   "source": [
    "#### What it Got Right ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090a985a-a152-4976-8f57-890e88ef99ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truthiness analysis\n",
    "correct_df = raw_df[raw_df[\"sbert_match_binary_correct\"]].reset_index()\n",
    "print(f\"Number correct: {len(correct_df):,}\")\n",
    "\n",
    "correct_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d228d188-ed22-42ca-985b-ddd9088ac94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis\n",
    "wrong_df = raw_df[raw_df[\"sbert_match_binary_correct\"] == False].reset_index()\n",
    "print(f\"Number wrong: {len(wrong_df):,}\")\n",
    "\n",
    "wrong_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67998e4-9729-4831-993d-aca757f9aa8c",
   "metadata": {},
   "source": [
    "## Structured Address Matching with Libpostal, PyTorch and `Sentence-BERT`\n",
    "\n",
    "Our next strategy will be to parse the addresses using Libpostal and then to encode them in a way that perserves the parsed staructure. Libposstal lowercases the address parts it produces, so we need to train a lowercase `SentenceTransformer` model. We will then load the weights of this pre-trained model as the embedding layers of a deep network architecture called `SentenceBERT` to build a classifier for pairs of addresses that can achieve better performance than fine-tuned sentence transformers and cosine similarity alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0715a288-d141-474a-a5f8-f5cbb7e0bd9a",
   "metadata": {},
   "source": [
    "### Fine-Tuning a Lowercase `SentenceTransformer`\n",
    "\n",
    "My first pass at this method did not work whatsoever - the performance of the matcher was abysmal. This was because Libpostal *lowercases* addresses when it parses them, and I did NOT do that to the training data on a first pass :) Once I did that and retrained below - things worked much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeb24d9-ef8c-4f39-a709-8630c749a809",
   "metadata": {},
   "source": [
    "#### Lowercase our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2c3ad6-8418-4fdc-98aa-e4187b648020",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_augment_results_df = augment_results_df.copy(deep=True)\n",
    "\n",
    "lower_augment_results_df[\"Address1\"] = lower_augment_results_df[\"Address1\"].str.lower()\n",
    "lower_augment_results_df[\"Address2\"] = lower_augment_results_df[\"Address2\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4cb876-ea2c-4133-a2ee-f4dbb635b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, tmp_df = train_test_split(lower_augment_results_df, test_size=0.2, shuffle=True)\n",
    "eval_df, test_df = train_test_split(tmp_df, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"sentence1\": train_df[\"Address1\"].tolist(),\n",
    "    \"sentence2\": train_df[\"Address2\"].tolist(),\n",
    "    \"label\": train_df[\"Label\"].tolist(),\n",
    "})\n",
    "\n",
    "eval_dataset = Dataset.from_dict({\n",
    "    \"sentence1\": eval_df[\"Address1\"].tolist(),\n",
    "    \"sentence2\": eval_df[\"Address2\"].tolist(),\n",
    "    \"label\": eval_df[\"Label\"].tolist(),\n",
    "})\n",
    "\n",
    "test_dataset = Dataset.from_dict({\n",
    "    \"sentence1\": test_df[\"Address1\"].tolist(),\n",
    "    \"sentence2\": test_df[\"Address2\"].tolist(),\n",
    "    \"label\": test_df[\"Label\"].tolist(),\n",
    "})\n",
    "\n",
    "print(f\"Training data:   {len(train_df):,}\")\n",
    "print(f\"Validation data: {len(eval_df):,}\")\n",
    "print(f\"Test data        {len(eval_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0af638c-048c-47a1-98bd-bdc670d9f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "SBERT_MODEL_LOWER = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "VARIANT = \"lowercase\"\n",
    "LOWER_MODEL_SAVE_NAME = (SBERT_MODEL_LOWER + \"-\" + VARIANT).replace(\"/\", \"-\")\n",
    "\n",
    "# Make sure these match the values in the data augmentation notebook for accurate loggging and reporting\n",
    "CLONES_PER_RUN = 100\n",
    "RUNS_PER_EXAMPLE = 2\n",
    "\n",
    "EPOCHS = 6\n",
    "BATCH_SIZE = 32\n",
    "PATIENCE = 2\n",
    "LEARNING_RATE = .00005\n",
    "DATASET_MULTIPLE = CLONES_PER_RUN * RUNS_PER_EXAMPLE\n",
    "SBERT_LOWER_OUTPUT_FOLDER = f\"data/fine-tuned-sbert-{LOWER_MODEL_SAVE_NAME}\"\n",
    "SAVE_EVAL_STEPS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450ff10b-99ce-4853-bcd2-37c17e4eb2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weights & Biases\n",
    "wandb.init(\n",
    "    entity=\"rjurney\",\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"libpostal-reborn\",\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"variant\": VARIANT,\n",
    "        \"dataset_multiple\": DATASET_MULTIPLE,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"patience\": PATIENCE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"sbert_model\": SBERT_MODEL_LOWER,\n",
    "        \"model_save_name\": LOWER_MODEL_SAVE_NAME,\n",
    "        \"sbert_output_folder\": SBERT_LOWER_OUTPUT_FOLDER,\n",
    "        \"save_eval_steps\": SAVE_EVAL_STEPS,\n",
    "    },\n",
    "    save_code=True,\n",
    "    save_checkpoint=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc7965c-5619-44a2-a07f-9646078db1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model_lower = SentenceTransformer(\n",
    "    SBERT_MODEL_LOWER,\n",
    "    device=device,\n",
    "    model_card_data=SentenceTransformerModelCardData(\n",
    "        language=\"en\",\n",
    "        license=\"apache-2.0\",\n",
    "        model_name=f\"{SBERT_MODEL_LOWER}-address-matcher-{VARIANT}\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05ff3a5-111d-4f1b-bdaa-2fc4c5859346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the evaluator\n",
    "binary_acc_evaluator = BinaryClassificationEvaluator(\n",
    "    sentences1=eval_dataset[\"sentence1\"],\n",
    "    sentences2=eval_dataset[\"sentence2\"],\n",
    "    labels=eval_dataset[\"label\"],\n",
    "    name=SBERT_MODEL_LOWER,\n",
    ")\n",
    "pd.DataFrame([binary_acc_evaluator(sbert_model_lower)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16800ca1-1f40-40de-9299-f8e88b0ee2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will rapidly train the embedding model. MultipleNegativesRankingLoss did not work.\n",
    "loss = losses.ContrastiveLoss(model=sbert_model_lower)\n",
    "\n",
    "sbert_args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=SBERT_LOWER_OUTPUT_FOLDER,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    warmup_ratio=0.1,\n",
    "    run_name=SBERT_MODEL_LOWER,\n",
    "    load_best_model_at_end=True,\n",
    "    save_steps=SAVE_EVAL_STEPS,\n",
    "    eval_steps=SAVE_EVAL_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    greater_is_better=False,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=sbert_model_lower,\n",
    "    args=sbert_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    loss=loss,\n",
    "    evaluator=binary_acc_evaluator,\n",
    "    compute_metrics=compute_sbert_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=PATIENCE)],\n",
    ")\n",
    "\n",
    "trainer.evaluate()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ce3743-1389-4736-8659-e50e15c5a90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best model checkpoint path: {trainer.state.best_model_checkpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0092c331-3ab7-436b-ac7e-0cf0bfd0406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([trainer.evaluate()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe81a1d-2171-4915-9e2c-e9a3a88d485f",
   "metadata": {},
   "source": [
    "### Save the Best Model\n",
    "\n",
    "Because we used `load_best_model_at_end=True`, our model is now the best one we fine-tuned. Save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16310db3-a9f5-48fd-a009-eb743e641bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(SBERT_LOWER_OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d671f99c-eee8-44df-a264-b943ca5e068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6739716e-4567-47e7-a49a-87228bc33f3f",
   "metadata": {},
   "source": [
    "### Rewrite our Matchers for Lowercase Duty\n",
    "\n",
    "Need two versions of these to compare the original with their new lowercase cousins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b1761b-001e-4fd8-b61a-7d31686bc9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sbert_compare_lower(address1: str, address2: str) -> float:\n",
    "    \"\"\"sbert_compare - sentence encode each address into a fixed-length text embedding.\n",
    "    Fixed-length means they can be compared with cosine similarity.\"\"\"\n",
    "    embedding1 = sbert_model_lower.encode(address1.lower())\n",
    "    embedding2 = sbert_model_lower.encode(address2.lower())\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    return 1 - distance.cosine(embedding1, embedding2)\n",
    "\n",
    "\n",
    "def sbert_match_lower(row: pd.Series) -> pd.Series:\n",
    "    \"\"\"sbert_match - SentenceTransformer address matching, float iytoyt\"\"\"\n",
    "    return sbert_compare_lower(row[\"Address1\"], row[\"Address2\"])\n",
    "\n",
    "\n",
    "def sbert_compare_binary_lower(address1: str, address2: str, threshold: float = 0.5) -> Literal[0, 1]:\n",
    "    \"\"\"sbert_match - compare and return a binary match\"\"\"\n",
    "    similarity = sbert_compare_lower(address1, address2)\n",
    "    return 1 if similarity >= threshold else 0\n",
    "\n",
    "\n",
    "def sbert_match_binary_lower(row: pd.Series, threshold: float = 0.5) -> pd.Series:\n",
    "    \"\"\"sbert_match_binary - SentenceTransformer address matching, binary output\"\"\"\n",
    "    return sbert_compare_binary_lower(row[\"Address1\"], row[\"Address2\"], threshold=threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94e3805-efb3-4677-97a6-03f469484ac0",
   "metadata": {},
   "source": [
    "### Evaluate ROC Curve to Determine Optimum Similarity Threshold\n",
    "\n",
    "We need to evaluate the ROC Curve of the F1 score to see what it should be set to for our lowercase model too. Recall that the `sbert_match_lower` function has a `threshold: float = 0.5` argument.\n",
    "\n",
    "#### Evaluate on our Augmented Test Dataset\n",
    "\n",
    "First we'll evaluate the ROC curve on our augmented test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245e39c8-4434-4e20-993e-b6dcf4b8bd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_df[\"Label\"]\n",
    "y_scores = test_df.apply(sbert_match_lower, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044d9937-5c3e-4e9a-aff2-e30aec095d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "\n",
    "# Compute F1 score for each threshold\n",
    "f1_scores = [f1_score(y_true, y_scores >= t) for t in thresholds]\n",
    "\n",
    "# Find the threshold that maximizes the F1 score\n",
    "best_threshold_index = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_threshold_index]\n",
    "best_f1_score = f1_scores[best_threshold_index]\n",
    "\n",
    "print(f'Best Threshold: {best_threshold}')\n",
    "print(f'Best F1 Score: {best_f1_score}')\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_scores)\n",
    "print(f'AUC-ROC: {roc_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3d1d6e-8ca0-4964-aa69-91e2e7ddffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for Seaborn\n",
    "pr_data = pd.DataFrame({\n",
    "    'Precision': precision[:-1],\n",
    "    'Recall': recall[:-1],\n",
    "    'F1 Score': f1_scores\n",
    "})\n",
    "\n",
    "# Plot Precision-Recall curve using Seaborn\n",
    "sns.lineplot(data=pr_data, x='Recall', y='Precision', marker='o')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Augmented Test Set Precision-Recall Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3a978d-6193-4968-99b3-65cfbd584c4e",
   "metadata": {},
   "source": [
    "### Plot a ROC Curve for our Gold Labeled Data\n",
    "\n",
    "We need to see the ROC Curve for our gold labeled data as well. We care more about performance on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb225e7-ad73-4236-b4ea-49fbb1da3bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = gold_df[\"Label\"]\n",
    "y_scores = gold_df.apply(sbert_match_lower, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d2c166-d6c2-481c-b573-f1c5ca6fd41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "\n",
    "# Compute F1 score for each threshold\n",
    "f1_scores = [f1_score(y_true, y_scores >= t) for t in thresholds]\n",
    "\n",
    "# Find the threshold that maximizes the F1 score\n",
    "best_threshold_index = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_threshold_index]\n",
    "best_f1_score = f1_scores[best_threshold_index]\n",
    "\n",
    "print(f'Best Threshold: {best_threshold}')\n",
    "print(f'Best F1 Score: {best_f1_score}')\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_scores)\n",
    "print(f'AUC-ROC: {roc_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50070e2-5c1e-4367-b5be-d1145d87e97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for Seaborn\n",
    "pr_data = pd.DataFrame({\n",
    "    'Precision': precision[:-1],\n",
    "    'Recall': recall[:-1],\n",
    "    'F1 Score': f1_scores\n",
    "})\n",
    "\n",
    "# Plot Precision-Recall curve using Seaborn\n",
    "sns.lineplot(data=pr_data, x='Recall', y='Precision', marker='o')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Gold Label Precision-Recall Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4958d260-90a9-417d-b200-df73e1869760",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_compare_lower(\n",
    "    \"nw 5th ave\",\n",
    "    \"northwest 5th avenue\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31be0a9-8281-4ed4-92e3-753a8f040ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_compare_lower(\n",
    "    \"101 market square, seattle, wa 98039\",\n",
    "    \"101 davis ct., seattle, wa 98039\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0388ebba-f964-41ce-ab37-c9c295a45806",
   "metadata": {},
   "source": [
    "## Structured Prediction with a `Sentence-BERT` Classifier\n",
    "\n",
    "Embeddings as a solution to this problem have a side-effect of optimizing an embedding for information retrieval... but they ignore the structure of parsed addresses. A deep network that is aware of it can perform better. Let's try out an implementation of the Sentence-BERT model, which was outlined by Nils Reimers and Iryna Gurevych in the original paper that created sentence tranformers, [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\n",
    "](https://arxiv.org/abs/1908.10084)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75976b16-ca9d-4005-9a51-e76a975f876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, tmp_df = train_test_split(augment_results_df, test_size=0.2, shuffle=True)\n",
    "eval_df, test_df = train_test_split(tmp_df, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "# Encode the addresses using [COL] / [VAL] special characters\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"sentence1\": [structured_encode_address(x) for x in train_df[\"Address1\"].tolist()],\n",
    "    \"sentence2\": [structured_encode_address(x) for x in train_df[\"Address2\"].tolist()],\n",
    "    \"label\": train_df[\"Label\"].tolist(),\n",
    "})\n",
    "\n",
    "# Encode the addresses using [COL] / [VAL] special characters\n",
    "eval_dataset = Dataset.from_dict({\n",
    "    \"sentence1\": [structured_encode_address(x) for x in eval_df[\"Address1\"].tolist()],\n",
    "    \"sentence2\": [structured_encode_address(x) for x in eval_df[\"Address2\"].tolist()],\n",
    "    \"label\": eval_df[\"Label\"].tolist(),\n",
    "})\n",
    "\n",
    "# Encode the addresses using [COL] / [VAL] special characters\n",
    "test_dataset = Dataset.from_dict({\n",
    "    \"sentence1\": [structured_encode_address(x) for x in test_df[\"Address1\"].tolist()],\n",
    "    \"sentence2\": [structured_encode_address(x) for x in test_df[\"Address2\"].tolist()],\n",
    "    \"label\": test_df[\"Label\"].tolist(),\n",
    "})\n",
    "\n",
    "print(f\"Training data:   {len(train_df):,}\")\n",
    "print(f\"Validation data: {len(eval_df):,}\")\n",
    "print(f\"Test data        {len(eval_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da913257-54a0-4220-9bca-5a341c9b5344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6f9841-6248-4e06-81b7-c5761fde354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SBERT_INPUT_MODEL = SBERT_LOWER_OUTPUT_FOLDER\n",
    "VARIANT = \"pre-trained-embeddings\"\n",
    "MODEL_SAVE_NAME = (\"Sentence-BERT\" + \"-\" + VARIANT).replace(\"/\", \"-\")\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "PATIENCE = 2\n",
    "LEARNING_RATE = 2e-5\n",
    "MODEL_OUTPUT_FOLDER = f\"data/{MODEL_SAVE_NAME}\"\n",
    "SAVE_EVAL_STEPS = 100\n",
    "\n",
    "COLUMN_SPECIAL_CHAR = \"[COL]\"\n",
    "VALUE_SPECIAL_CHAR = \"[VAL]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb214a-2cbb-48d2-9e42-307aab992516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weights & Biases\n",
    "wandb.init(\n",
    "    entity=\"rjurney\",\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"libpostal-reborn\",\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"model\": \"Sentence-BERT\",\n",
    "        \"variant\": VARIANT,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"patience\": PATIENCE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"sbert_input_model\": SBERT_INPUT_MODEL,\n",
    "        \"model_output_folder\": MODEL_OUTPUT_FOLDER,\n",
    "        \"save_eval_steps\": SAVE_EVAL_STEPS,\n",
    "        \"model_save_name\": MODEL_SAVE_NAME,\n",
    "    },\n",
    "    save_code=True,\n",
    "    save_checkpoint=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249d7242-7fc9-4c1c-b7cc-2b4347355502",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceBERT(nn.Module):\n",
    "    def __init__(self, model_name=SBERT_INPUT_MODEL, dim=384):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # Freeze the transformer layers\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Update the input dimension of the FFNN to account for the new features\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(3 * dim, dim),  \n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(dim, 1)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_pool(token_embeds, attention_mask):\n",
    "        in_mask = attention_mask.unsqueeze(-1).expand(token_embeds.size()).float()\n",
    "        pool = torch.sum(token_embeds * in_mask, 1) / torch.clamp(in_mask.sum(1), min=1e-9)\n",
    "        return pool\n",
    "\n",
    "    def _check_similarity(self, a, b, mask_a, mask_b):\n",
    "        u = self.model(a, attention_mask=mask_a)[0]\n",
    "        v = self.model(b, attention_mask=mask_b)[0]\n",
    "        u = self.mean_pool(u, mask_a)\n",
    "        v = self.mean_pool(v, mask_b)\n",
    "\n",
    "        # Construct ( u, v, |u-v| ) same as sbert paper\n",
    "        uv = torch.sub(u, v)\n",
    "        uv_abs = torch.abs(uv)\n",
    "        x = torch.cat([u, v, uv_abs], dim=-1)\n",
    "\n",
    "        # From (dim, 3) -> (dim, 2)\n",
    "        x = self.sigmoid(self.ffnn(x).float())\n",
    "        return x\n",
    "\n",
    "    def check_similarity(self, a, b):\n",
    "        encoded_a = self.tokenizer(a, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        encoded_b = self.tokenizer(b, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        a = encoded_a[\"input_ids\"].to(self.model.device)\n",
    "        b = encoded_b[\"input_ids\"].to(self.model.device)\n",
    "        mask_a = encoded_a[\"attention_mask\"].to(self.model.device)\n",
    "        mask_b = encoded_b[\"attention_mask\"].to(self.model.device)\n",
    "        with torch.no_grad():\n",
    "            return self._check_similarity(a, b, mask_a, mask_b)\n",
    "\n",
    "    def forward(self, input_ids_a, input_ids_b, attention_mask_a=None, attention_mask_b=None, labels=None):\n",
    "        logits = self._check_similarity(input_ids_a, input_ids_b, attention_mask_a, attention_mask_b)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.BCELoss()\n",
    "            labels = labels.float().unsqueeze(1)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            loss = loss_fct(probs, labels)\n",
    "            return {\"loss\": loss, \"logits\": logits}\n",
    "        return {\"logits\": logits}\n",
    "\n",
    "    def predict(self, a: str, b: str):\n",
    "        with torch.no_grad():\n",
    "            probabilities = self.check_similarity(a, b)\n",
    "            predicted_class = (probabilities > 0.5).long().item()\n",
    "            return predicted_class, probabilities.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15494649-c8bc-495c-9a10-7516c315b6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = SentenceBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee830c6-cea1-4113-b398-827e5f101643",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(lambda x: tokenize_function(x, classifier_model), batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(lambda x: tokenize_function(x, classifier_model), batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(lambda x: tokenize_function(x, classifier_model), batched=True)\n",
    "\n",
    "tokenized_train_dataset = format_dataset(tokenized_train_dataset)\n",
    "tokenized_eval_dataset = format_dataset(tokenized_eval_dataset)\n",
    "tokenized_test_dataset = format_dataset(tokenized_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e650246e-27b7-4135-bbf6-8320a8858b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_OUTPUT_FOLDER,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    warmup_ratio=0.1,\n",
    "    run_name=MODEL_SAVE_NAME,\n",
    "    load_best_model_at_end=True,\n",
    "    save_steps=SAVE_EVAL_STEPS,\n",
    "    eval_steps=SAVE_EVAL_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    greater_is_better=False,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    use_mps_device=True,\n",
    ")\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\").float()\n",
    "        input_ids_a = inputs.get(\"input_ids_a\")\n",
    "        attention_mask_a = inputs.get(\"attention_mask_a\")\n",
    "        input_ids_b = inputs.get(\"input_ids_b\")\n",
    "        attention_mask_b = inputs.get(\"attention_mask_b\")\n",
    "        outputs = model(input_ids_a, input_ids_b, attention_mask_a, attention_mask_b)\n",
    "        loss = self.loss_fct(outputs['logits'].view(-1), labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=classifier_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    compute_metrics=compute_classifier_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=PATIENCE)],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ce2275-6f76-4454-863a-1a3b8adcd829",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best model checkpoint path: {trainer.state.best_model_checkpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693e3cc0-8844-43af-a738-cc25f7778ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "pd.DataFrame([trainer.evaluate()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a393084f-aa67-48ad-ad08-93aaea71fb3b",
   "metadata": {},
   "source": [
    "### Save the Best Model\n",
    "\n",
    "Because we used `load_best_model_at_end=True`, our model is now the best one we fine-tuned. Save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3812726-f59e-467a-9945-d7e9fcf2f2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(MODEL_OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70abb3b9-a9e3-4280-a659-728aeec6c52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_custom_model(classifier_model, \"data/classifier_model_for_tanmoy.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22cd20c-61be-47c4-9eba-eac14a1cc23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abab52e-f77c-4fe9-8c99-9bc8d77adcb9",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af34935-68fc-4adf-82bd-e91c4bfe8583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_custom_model(model_cls, load_path, device=\"cpu\"):\n",
    "    # Initialize your custom model\n",
    "    custom_model = model_cls(model_name=load_path)\n",
    "\n",
    "    # Load the base BERT model and tokenizer\n",
    "    custom_model.model = AutoModel.from_pretrained(load_path)\n",
    "    custom_model.tokenizer = AutoTokenizer.from_pretrained(load_path)\n",
    "\n",
    "    # Load the full state dict\n",
    "    state_dict = torch.load(\n",
    "        os.path.join(load_path, \"full_model_state_dict.pt\"), map_location=device\n",
    "    )\n",
    "    custom_model.load_state_dict(state_dict)\n",
    "\n",
    "    return custom_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8112c70-6bbe-4ee1-bc0f-cacc51aa472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier_model = load_custom_model(SentenceBERT, \"data/classifier_model_for_tanmoy.pt\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b6b68f-c85f-4753-ace8-0a74e3471d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.predict(\"3413 Sean Way, Lawrenceville, GA 30044\", \"3413 Sean Way, Lawrenceville, GA 30044\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387a0065-5412-48e3-b5cb-d146aa4cd528",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.predict(\"101 Oak Ct.,\", \"101 Oak Street, Philadelphia, PA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4265009-bf3e-467c-84fd-27d202cf5c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.predict(\"101 Oak Pl.\", \"101 Oak Place\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3d4025-ec9b-45cd-a3fd-b764ab1b9920",
   "metadata": {},
   "source": [
    "### Probability and Boolean Prediction Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808a4f57-c875-4ec2-9bf1-4fc2a1971afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_match(row: pd.Series) -> pd.Series:\n",
    "    \"\"\"classifier_match - Sentence-BERT address matching, float output\"\"\"\n",
    "    return classifier_model.predict(row[\"Address1\"], row[\"Address2\"])[1]\n",
    "\n",
    "\n",
    "def classifier_match_boolean(row: pd.Series, threshold=0.5) -> pd.Series:\n",
    "    \"\"\"classifier_match_binary - Sentence-BERT address matching, boolean output\"\"\"\n",
    "    return 1 if classifier_model.predict(row[\"Address1\"], row[\"Address2\"])[1] > threshold else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5f79c9-a93e-4d5c-ae1f-bc819e89847a",
   "metadata": {},
   "source": [
    "### Synthetic Data Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6d6951-b186-40f0-913b-2e209b376439",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_df[\"Label\"]\n",
    "y_scores = test_df.apply(classifier_match, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d13decc-0983-4dac-936e-1c38c9e4eadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "\n",
    "# Compute F1 score for each threshold\n",
    "f1_scores = [f1_score(y_true, y_scores >= t) for t in thresholds]\n",
    "\n",
    "# Find the threshold that maximizes the F1 score\n",
    "best_threshold_index = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_threshold_index]\n",
    "best_f1_score = f1_scores[best_threshold_index]\n",
    "\n",
    "print(f'Best Threshold: {best_threshold}')\n",
    "print(f'Best F1 Score: {best_f1_score}')\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_scores)\n",
    "print(f'AUC-ROC: {roc_auc}')\n",
    "\n",
    "# Create a DataFrame for Seaborn\n",
    "pr_data = pd.DataFrame({\n",
    "    'Precision': precision[:-1],\n",
    "    'Recall': recall[:-1],\n",
    "    'F1 Score': f1_scores\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b571184-2b27-4a40-9969-02fec11c4e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall curve using Seaborn\n",
    "sns.lineplot(data=pr_data, x='Recall', y='Precision', marker='o')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Augmented Test Set Precision-Recall Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f55d107-561c-4bf7-afe4-f78172c16ae2",
   "metadata": {},
   "source": [
    "### Gold Label Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b262d8-346d-4d14-8947-7c3f61302c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = gold_df[\"Label\"]\n",
    "y_scores = gold_df.apply(classifier_match, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af49aa98-4a18-4389-bf00-ee6d10948dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "\n",
    "# Compute F1 score for each threshold\n",
    "f1_scores = [f1_score(y_true, y_scores >= t) for t in thresholds]\n",
    "\n",
    "# Find the threshold that maximizes the F1 score\n",
    "best_threshold_index = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_threshold_index]\n",
    "best_f1_score = f1_scores[best_threshold_index]\n",
    "\n",
    "print(f'Best Threshold: {best_threshold}')\n",
    "print(f'Best F1 Score: {best_f1_score}')\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_scores)\n",
    "print(f'AUC-ROC: {roc_auc}')\n",
    "\n",
    "# Create a DataFrame for Seaborn\n",
    "pr_data = pd.DataFrame({\n",
    "    'Precision': precision[:-1],\n",
    "    'Recall': recall[:-1],\n",
    "    'F1 Score': f1_scores\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b672998-93d9-4091-959d-203002de9a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall curve using Seaborn\n",
    "sns.lineplot(data=pr_data, x='Recall', y='Precision', marker='o')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Gold Label Precision-Recall Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e135a0db-3cdb-4d1e-a0ce-9ccf97ca2b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df, grouped_df = gold_label_report(\n",
    "    gold_df,\n",
    "    [\n",
    "        # sbert_match_binary,\n",
    "        classifier_match_boolean,\n",
    "    ],\n",
    "    threshold=best_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087f4277-693a-44d1-b7b2-81895d7b4100",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cde90e-b2db-401a-8a7b-0dd3e24edb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truthiness analysis\n",
    "correct_df = raw_df[raw_df[\"classifier_match_boolean_correct\"]].reset_index(drop=True)\n",
    "print(f\"Number correct: {len(correct_df):,}\")\n",
    "\n",
    "correct_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ed830e-88ff-4c92-addb-ea73f60a5edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis\n",
    "wrong_df = raw_df[raw_df[\"classifier_match_boolean_correct\"] == False].reset_index()\n",
    "print(f\"Number wrong: {len(wrong_df):,}\")\n",
    "\n",
    "wrong_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54205060-4522-4bea-840a-fc0aede13d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.predict(\n",
    "    \"101 Oak Lane, Atlanta, GA 30308\",\n",
    "    \"102 Oak Lane, Atlanta, GA 30308\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37beb453-3084-4e3f-b635-c4f9882f12e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.predict(\n",
    "    \"101 Oak Lane, Macon, GA 30308\",\n",
    "    \"101 Oak Lane, Atlanta, GA 30408\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29452c9c-abb1-49b7-819e-b0a42ede9440",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.predict(\n",
    "    \"101 Oak Lane, Atlanta, GA 30308\",\n",
    "    \"101 Oak Ln., Atlanta, GA 30308\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d97d1c0-4e93-47a2-b52d-7c5be5ce555d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.predict(\n",
    "    \"3413 Sean Way, Lawrenceville, GA 30044\",\n",
    "    \"1202 Oak Rd., Lawrenceville, GA 30304\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4972bc19-ba6b-4e0b-80cc-80baf013ce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.predict(\n",
    "    \"3413 Sean Way, Lawrenceville, GA 30044\",\n",
    "    \"3413 Sean Way, Lawrenceville, GA 30044, USA\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
