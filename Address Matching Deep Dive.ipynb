{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d0f6845-e53f-4e9b-be24-0e376269f522",
   "metadata": {},
   "source": [
    "# Address Matching Deep Dive\n",
    "\n",
    "This notebook experiments with different ways of comparing addresses in order to demonstrate the power of parsed address comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feab8c8-a116-40f9-a882-5a932140af26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from numbers import Number\n",
    "from typing import Callable, Dict, List, Literal, Sequence, Tuple, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "import pycountry\n",
    "import pytest\n",
    "import random\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import wandb\n",
    "from datasets import Dataset\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "from postal.parser import parse_address\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import accuracy_score, precision_recall_curve, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers import InputExample, SentenceTransformer, SentencesDataset, SentenceTransformerTrainer, losses\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, SimilarityFunction, BinaryClassificationEvaluator\n",
    "from sentence_transformers.model_card import SentenceTransformerModelCardData\n",
    "from sentence_transformers.training_args import BatchSamplers, SentenceTransformerTrainingArguments\n",
    "from tenacity import retry\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.autonotebook import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel, EarlyStoppingCallback, TrainingArguments, Trainer\n",
    "from transformers.integrations import WandbCallback\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "\n",
    "from utils import (\n",
    "    augment_gold_labels,\n",
    "    compute_sbert_metrics,\n",
    "    compute_classifier_metrics,\n",
    "    gold_label_report,\n",
    "    preprocess_logits_for_metrics,\n",
    "    to_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e508b00c-0011-4845-a4d6-cd131ec5b6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stderr, level=logging.ERROR)\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ba8945-a02c-4661-a865-c03ee6eecebe",
   "metadata": {},
   "source": [
    "#### Configure Weights & Biases\n",
    "\n",
    "`wandb` needs some environment variables to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9491984a-c9fb-40d5-8fb0-743b77d15c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_LOG_MODEL\"] = \"end\"\n",
    "os.environ[\"WANDB_WATCH\"] = \"all\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"libpostal-reborn\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"false\"\n",
    "os.environ[\"WANDB_IGNORE_GLOBS\"] = \".env\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e1d411-e2a5-4824-9309-ff50861a0e19",
   "metadata": {},
   "source": [
    "#### Configure Huggingface APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d0d44b-c38a-48ca-a3b1-895b5b8c320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_ENDPOINT\"] = \"https://huggingface.co/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7462e0-b3d1-4d76-bd7a-803cbd2f3446",
   "metadata": {},
   "source": [
    "#### Configure Huggingface APIs\n",
    "\n",
    "Squash any warnings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcde550-79a6-4333-967e-c80d555de5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42913e46-e5db-4eff-9410-5a8449d4cbf6",
   "metadata": {},
   "source": [
    "#### Configure Pandas to Show More Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d4cb15-b201-4f34-8917-d292dd2a1876",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad42226-1abc-4c29-b68d-659d330ba1f9",
   "metadata": {},
   "source": [
    "### Use CUDA or MPS if Avaialable\n",
    "\n",
    "CPU training and even inference with sentence transformers and deep learning models is quite slow. Since all machine learning in this library is based on [PyTorch](https://pytorch.org/get-started/locally/), we can assign all ML operations to a GPU in this one block of code. Otherwise we default to CPU without acceleration. The notebook is still workable in this mode, you just may need to grab a cup of tea or coffee while you wait for it to train the Sentence-BERT model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1051aa37-26b7-496a-8182-f10cfa1f5f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for CUDA or MPS availability and set the device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    logger.debug(\"Using Apple GPU acceleration\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    logger.debug(\"Using NVIDIA CUDA GPU acceleration\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    logger.debug(\"Using CPU for ML\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921a41eb-4e6b-4d64-a3b4-46013524e37c",
   "metadata": {},
   "source": [
    "### Use Weights & Biases for Logging Metrics\n",
    "\n",
    "Weights & Biases has a free account for individuals with public projects. Using it will produce charts during our training runs that anyone can view. You can create your own project for this notebook and login with that key to log your own training runs.\n",
    "\n",
    "You may need to run the following command from your shell before the next cell, otherwise you will have to paste your project key into the \n",
    "\n",
    "```bash\n",
    "wandb login\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98cdf2f-fcb9-495e-9201-80f9e07d9070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to wandb. Comment out if you already haven't via `wandb login` from a CLI\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85701b19-a5e1-42c4-9ace-8d88d96f893f",
   "metadata": {},
   "source": [
    "## Implementing a Structured Address Matcher\n",
    "\n",
    "Let's start our exercise by using the structured address data provided by [Libpostal](https://github.com/openvenues/libpostal) to parse them for matching. We write a function for each address part to deal with missing fields without duplicating a lot of logic.\n",
    "\n",
    "We start with something quite literal and basic. We'll improve it as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44031098-15b5-45ef-b1d0-8a51a9838bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_match_address(address1: str, address2: str) -> Literal[0, 1]:\n",
    "    \"\"\"parse_match_address implements address matching using the precise, parsed structure of addresses.\"\"\"\n",
    "    address1 = to_dict(parse_address(address1))\n",
    "    address2 = to_dict(parse_address(address2))\n",
    "\n",
    "    def match_road(address1: Dict, address2: Dict) -> Literal[0, 1]:\n",
    "        \"\"\"match_road - literal road matching, negative if either lacks a road\"\"\"\n",
    "        if (\"road\" in address1) and (\"road\" in address2):\n",
    "            if address1[\"road\"] == address2[\"road\"]:\n",
    "                logger.debug(\"road match\")\n",
    "                return 1\n",
    "            else:\n",
    "                logger.debug(\"road mismatch\")\n",
    "                return 0\n",
    "        logger.debug(\"road mismatch\")\n",
    "        return 0\n",
    "\n",
    "    def match_house_number(address1: Dict, address2: Dict) -> Literal[0, 1]:\n",
    "        \"\"\"match_house_number - literal house number matching, negative if either lacks a house_number\"\"\"\n",
    "        if (\"house_number\" in address1) and (\"house_number\" in address2):\n",
    "            if address1[\"house_number\"] == address2[\"house_number\"]:\n",
    "                logger.debug(\"house_number match\")\n",
    "                return 1\n",
    "            else:\n",
    "                logger.debug(\"house_number mismatch\")\n",
    "                return 0\n",
    "        logger.debug(\"house_number mistmatch\")\n",
    "        return 0\n",
    "\n",
    "    def match_unit(address1: Dict, address2: Dict) -> Literal[0, 1]:\n",
    "        \"\"\"match_unit - note a missing unit in both is a match\"\"\"\n",
    "        if \"unit\" in address1:\n",
    "            if \"unit\" in address2:\n",
    "                logger.debug(\"unit match\")\n",
    "                return 1 if (address1[\"unit\"] == address2[\"unit\"]) else 0\n",
    "            else:\n",
    "                logger.debug(\"unit mismatch\")\n",
    "                return 0\n",
    "        if \"unit\" in address2:\n",
    "            if \"unit\" in address1:\n",
    "                logger.debug(\"unit match\")\n",
    "                return 1 if (address1[\"unit\"] == address2[\"unit\"]) else 0\n",
    "            else:\n",
    "                logger.debug(\"unit mismatch\")\n",
    "                return 0\n",
    "        # Neither address has a unit, which is a default match\n",
    "        return 1\n",
    "\n",
    "    def match_postcode(address1: Dict, address2: Dict) -> Literal[0, 1]:\n",
    "        \"\"\"match_postcode - literal matching, negative if either lacks a postal code\"\"\"\n",
    "        if (\"postcode\" in address1) and (\"postcode\" in address2):\n",
    "            if address1[\"postcode\"] == address2[\"postcode\"]:\n",
    "                logger.debug(\"postcode match\")\n",
    "                return 1\n",
    "            else:\n",
    "                logger.debug(\"postcode mismatch\")\n",
    "                return 0\n",
    "        logger.debug(\"postcode mismatch\")\n",
    "        return 0\n",
    "\n",
    "    def match_country(address1: Dict, address2: Dict) -> Literal[0, 1]:\n",
    "        \"\"\"match_country - literal country matching - pass if both don't have one\"\"\"\n",
    "        if (\"country\" in address1) and (\"country\" in address2):\n",
    "            if address1[\"country\"] == address2[\"country\"]:\n",
    "                logger.debug(\"country match\")\n",
    "                return 1\n",
    "            else:\n",
    "                logger.debug(\"country mismatch\")\n",
    "                return 0\n",
    "        # One or none countries should match\n",
    "        logger.debug(\"country match\")\n",
    "        return 1\n",
    "\n",
    "    # Combine the above to get a complete address matcher\n",
    "    if (\n",
    "        match_road(address1, address2)\n",
    "        and match_house_number(address1, address2)\n",
    "        and match_unit(address1, address2)\n",
    "        and match_postcode(address1, address2)\n",
    "        and match_country(address1, address2)\n",
    "    ):\n",
    "        logger.debug(\"overall match\")\n",
    "        return 1\n",
    "    else:\n",
    "        logger.debug(\"overall mismatch\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f67113-6f5f-45cb-9f4c-4ee0a4362de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yup down to house_number ...\n",
    "parse_match_address(\n",
    "    \"3413 Sean Way, Lawrenceville, GA 30044\",\n",
    "    \"3413 Sean Way Lawrenceville, GA, 30044\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea361bd-56df-4980-a060-1d7e743af38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz.ratio(\n",
    "    \"3413 Sean Way, Lawrenceville, GA 30044\",\n",
    "    \"3413 Sean Way Lawrenceville, GA, 30044\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797ace98-a912-4c45-b178-6c6201f58bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yup down to unit ...\n",
    "parse_match_address(\n",
    "    \"120 Ralph McGill Blvd, Apt 101, Atlanta, GA 30308, USA\",\n",
    "    \"120 Ralph McGill Blvd, Apt 101, Atlanta GA 30308\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caf28fb-64e1-447f-a996-6d0d9e67bb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz.ratio(\n",
    "    \"120 Ralph McGill Blvd, Apt 101, Atlanta, GA 30308, USA\",\n",
    "    \"120 Ralph McGill Blvd, Apt 101, Atlanta GA 30308\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b949ebe-8889-498c-a601-904c2c5c1433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nope if only one uses an abbreviation ...\n",
    "parse_match_address(\n",
    "    \"120 Ralph McGill Blvd, Apt 333, Atlanta, GA 30308\",\n",
    "    \"120 Ralph McGill Boulevard, Apt 333, Atlanta, GA 30308\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108b88c9-9aca-4a13-80f9-2c88dbe77afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz.ratio(\n",
    "    \"120 Ralph McGill Blvd, Apt 333, Atlanta, GA 30308\",\n",
    "    \"120 Ralph McGill Boulevard, Apt 333, Atlanta, GA 30308\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498ab13c-1457-4c4c-95ff-2f691f697738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nope if one character in the streetname is off ...\n",
    "parse_match_address(\n",
    "    \"120 Ralp McGill Blvd, Apt 333, Atlanta, GA 30308\",\n",
    "    \"120 Ralph McGill Blvd, Apt 333, Atlanta, GA 30308\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6681fa67-bdf4-43b0-984d-8c91e5cbf902",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz.ratio(\n",
    "    \"120 Ralp McGill Blvd, Apt 333, Atlanta, GA 30308\",\n",
    "    \"120 Ralph McGill Blvd, Apt 333, Atlanta, GA 30308\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f1a4c5-0bcd-4c52-b5d7-7be7ad1932ea",
   "metadata": {},
   "source": [
    "### Literal is Too Precise!\n",
    "\n",
    "While it is useful to parse addresses and implement literal matching logic as we did above, as the third example indicates, an abbreviation or a single typo results in a mistmatch. We're going to write a more complex, approximate logical matcher below using string distance and text embeddings.\n",
    "\n",
    "Depending on your application you might relax this criteria to include corner cases such as missing postcodes. Before we get into that, let's create some training and evaluation data using hand-labeled data with an LLM data augmentation strategy to generate a lot of labeled records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15895b85-1d0f-441c-9d20-0849b3db3b22",
   "metadata": {},
   "source": [
    "## Data Augmentation with the OpenAI GPT4o API\n",
    "\n",
    "Run the sister notebook [Address Data Augmentation.ipynb](Address%20Matching%20Deep%20Dive.ipynb) before procceeding to create your training data via minimal manual labeling and programmatic data labeling for data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833e8045-5cd3-4b5a-9cd3-afc65965062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df = pd.read_csv(\"data/gold.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6382fc3-ca84-4d17-9d74-354ab798d21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to start from here and not run the data augmentation pipeline again...\n",
    "augment_results_df = pd.read_parquet(\"data/training.5.parquet\")\n",
    "\n",
    "augment_results_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2b2e4e-0c28-47ce-8b3c-7c1dac0062f5",
   "metadata": {},
   "source": [
    "### Data Augmentation Complete!\n",
    "\n",
    "Starting by hand labeling under 100 records and iterating a few times on data augmentation instructions for GPT4o, we have multiplied them by many times to get almost 10,000 synthetic records! This is enough to fine-tune a `SentenceTransformer` or semantic text similarity classifier model. GPT4o is a powerful tool for data augmentation! This can work for a variety of problems.\n",
    "\n",
    "LLM based data augmentation is a powerful tool for your data labeling toolbox."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423992c9-bd0c-4e85-b5ce-cbf14a1d5356",
   "metadata": {},
   "source": [
    "# Comparing Different Approaches to Address Matching\n",
    "\n",
    "Now we're going to compare the following methods of address matching:\n",
    "\n",
    "1) String Distance - we'll use PyPI library [fuzzywuzzy](https://pypi.org/project/fuzzywuzzy/0.6.1/) to compute the Levenshtein ratio and partial ratio of how many edits are required to match the address strings. There are times this works well, and there are times it couldn't be more wrong.\n",
    "2) Text Embeddings - we'll use transfer learning to load an existing [SentenceTransformer](https://sbert.net) model to sentence encode pairs of addresses to create fixed-length embeddings for each address and then compute a similarity score via [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity). This won't work without fine-tuning, so we fine-tune the model to the task.\n",
    "3) Deep Matching Model - We'll train a deep semantic textual similarity classification model that will use Siamese BERT networks as in [Sentence-BERT](https://arxiv.org/abs/1908.10084) to classify address pairs as matching or not matching.\n",
    "\n",
    "## Imprecise Country Matching with `pycountry`\n",
    "\n",
    "The structured address has fields that each have their own semantics. Tools specific to a field can help match address components.\n",
    "\n",
    "If you have any valid ISO nation abbreviation or long form name, [pycountry](https://pypi.org/project/pycountry/) is a PyPi module that can retrieve the actual country for it. This enables efficient comparison. Let's use that to convert the mismatched Singapore references in these records to the same entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5668a3-cd66-4eb1-a9b3-b029dc045f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_country_names(country1: str, country2: str) -> Literal[0, 1]:\n",
    "    \"\"\"match_country_strings - compare and match varying country formats using pycountry\"\"\"\n",
    "\n",
    "    # Remove any punctuation from the country\n",
    "    def remove_punctuation(country: str) -> str:\n",
    "        # Use re.sub to replace all punctuation characters with an empty string\n",
    "        return re.sub(r\"[^\\w\\s]\", \"\", country)\n",
    "\n",
    "    def multi_lookup(**kwargs):\n",
    "        \"\"\"Try each key until we retrieve a result\"\"\"\n",
    "        for arg, value in kwargs.items():\n",
    "            result = pycountry.countries.get(**{arg: value})\n",
    "            if result:\n",
    "                return result\n",
    "\n",
    "    def get_args(country: str):\n",
    "        \"\"\"Compose pycountries.countries.get arguments dict based on length of country string\"\"\"\n",
    "        args = {}\n",
    "        if country and len(country) == 2:\n",
    "            args[\"alpha_2\"] = country\n",
    "        elif country and len(country) == 3:\n",
    "            args[\"alpha_3\"] = country\n",
    "        elif country:\n",
    "            args[\"name\"] = country\n",
    "            args[\"common_name\"] = country\n",
    "            args[\"official_name\"] = country\n",
    "        return args\n",
    "\n",
    "    try:\n",
    "        pycountry1 = multi_lookup(**get_args(remove_punctuation(country1)))\n",
    "        pycountry2 = multi_lookup(**get_args(remove_punctuation(country2)))\n",
    "\n",
    "        return 1 if pycountry1.name == pycountry2.name else 0\n",
    "    except AttributeError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f275580d-a5ed-40b2-a27f-5d7134e56eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_country_names(\"sg\", \"singapore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a967b3-ada6-4b8e-b526-671460f0f81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_country_names(\"usa\", \"united states\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a116cb-05c4-4bcd-8c35-3e16f12621ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Didn't match until I added and called remove_punctuation(country: str) -> str\n",
    "match_country_names(\"U.S.A.\", \"United States of America\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7774066a-5e75-4420-ba88-ecb39cb7d828",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_country_names(\"USA\", \"MEX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089d99c3-61ec-442e-97ad-b7f826186579",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_country_names(\"United States\", \"United Mexican States\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df92059-6211-4044-973f-844a04682d96",
   "metadata": {},
   "source": [
    "### Country Parsing in Structured Matching\n",
    "\n",
    "Let's use our new method `match_pycountry(country1: str, country2: str) -> Literal[0, 1]` matcher to improve our original structured matcher. This will allow it to contain varying country formats and still match. This makes the matcher more robust. \n",
    "\n",
    "In order to make this work we have to refactor our code to create matching functions for each field. Note that we are leaving out matching states, as they aren't required if the road name, number, unit and postal code match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9ad90b-9524-4cd4-b0da-4c0a0b1e7f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_match_address_country(address1: str, address2: str) -> Literal[0, 1]:\n",
    "    \"\"\"parse_match_address_country implements address matching like parse_match_address() but with pycountry country matching\"\"\"\n",
    "    address1 = to_dict(parse_address(address1))\n",
    "    address2 = to_dict(parse_address(address2))\n",
    "\n",
    "    def match_road(address1: str, address2: str) -> Literal[0, 1]:\n",
    "        \"\"\"match_road - literal road matching, negative if either lacks a road\"\"\"\n",
    "        if (\"road\" in address1) and (\"road\" in address2):\n",
    "            if address1[\"road\"] == address2[\"road\"]:\n",
    "                logger.debug(\"road match\")\n",
    "                return 1\n",
    "            else:\n",
    "                logger.debug(\"road mismatch\")\n",
    "                return 0\n",
    "        logger.debug(\"road mismatch\")\n",
    "        return 0\n",
    "\n",
    "    def match_house_number(address1: str, address2: str) -> Literal[0, 1]:\n",
    "        \"\"\"match_house_number - literal house number matching, negative if either lacks a house_number\"\"\"\n",
    "        if (\"house_number\" in address1) and (\"house_number\" in address2):\n",
    "            if address1[\"house_number\"] == address2[\"house_number\"]:\n",
    "                logger.debug(\"house_number match\")\n",
    "                return 1\n",
    "            else:\n",
    "                logger.debug(\"house_number mismatch\")\n",
    "                return 0\n",
    "        logger.debug(\"house_number mistmatch\")\n",
    "        return 0\n",
    "\n",
    "    def match_unit(address1: str, address2: str) -> Literal[0, 1]:\n",
    "        \"\"\"match_unit - note a missing unit in both is a match\"\"\"\n",
    "        if \"unit\" in address1:\n",
    "            if \"unit\" in address2:\n",
    "                logger.debug(\"unit match\")\n",
    "                return 1 if (address1[\"unit\"] == address2[\"unit\"]) else 0\n",
    "            else:\n",
    "                logger.debug(\"unit mismatch\")\n",
    "                return 0\n",
    "        if \"unit\" in address2:\n",
    "            if \"unit\" in address1:\n",
    "                logger.debug(\"unit match\")\n",
    "                return 1 if (address1[\"unit\"] == address2[\"unit\"]) else 0\n",
    "            else:\n",
    "                logger.debug(\"unit mismatch\")\n",
    "                return 0\n",
    "        # Neither address has a unit, which is a default match\n",
    "        return 1\n",
    "\n",
    "    def match_postcode(address1: str, address2: str) -> Literal[0, 1]:\n",
    "        \"\"\"match_postcode - literal matching, negative if either lacks a postal code\"\"\"\n",
    "        if (\"postcode\" in address1) and (\"postcode\" in address2):\n",
    "            if address1[\"postcode\"] == address2[\"postcode\"]:\n",
    "                logger.debug(\"postcode match\")\n",
    "                return 1\n",
    "            else:\n",
    "                logger.debug(\"postcode mismatch\")\n",
    "                return 0\n",
    "        logger.debug(\"postcode mismatch\")\n",
    "        return 0\n",
    "\n",
    "    def match_country(address1: str, address2: str) -> Literal[0, 1]:\n",
    "        \"\"\"match_country - semantic country matching with pycountry via match_country_names(country1, country2)\"\"\"\n",
    "        if (\"country\" in address1) and (\"country\" in address2):\n",
    "            if match_country_names(address1[\"country\"], address2[\"country\"]):\n",
    "                logger.debug(\"country match\")\n",
    "                return 1\n",
    "            else:\n",
    "                logger.debug(\"country mismatch\")\n",
    "                return 0\n",
    "        # One or none countries should match\n",
    "        logger.debug(\"country match\")\n",
    "        return 1\n",
    "\n",
    "    # Combine the above to get a complete address matcher\n",
    "    if (\n",
    "        match_road(address1, address2)\n",
    "        and match_house_number(address1, address2)\n",
    "        and match_unit(address1, address2)\n",
    "        and match_postcode(address1, address2)\n",
    "        # Our only non-exact match - default to 1, match\n",
    "        and match_country(address1, address2)\n",
    "    ):\n",
    "        logger.debug(\"overall match\")\n",
    "        return 1\n",
    "    else:\n",
    "        logger.debug(\"overall mismatch\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f807c16a-02db-4246-b696-ca1ff4cd9372",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_match_address_country(\n",
    "    \"100 Roxas Blvd, Ermita, Manila, 1000 Metro Manila, PH\",\n",
    "    \"100 Roxas Blvd, Ermita, Manila, 1000 Metro Manila, Republic of the Philippines\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbc2cc2-39c6-4570-9cb5-55664e44cdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defaults to match if no countries are provided\n",
    "parse_match_address_country(\n",
    "    \"100 King St W, Toronto, ON M5X 1A9\",\n",
    "    \"100 King St W, Toronto, ON M5X 1A9\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27403f50-900b-44c0-bae0-58ed27ccd3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defaults to match if only one address has country\n",
    "parse_match_address_country(\n",
    "    \"100 King St W, Toronto, ON M5X 1A9\",\n",
    "    \"100 King St W, Toronto, ON M5X 1A9, Canada\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783b75eb-86ee-4674-928e-474e00e7e2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify mismatch\n",
    "parse_match_address_country(\n",
    "    \"Bosque de Chapultepec I Secc, Miguel Hidalgo, 11850 Ciudad de México, CDMX, Mexico\",\n",
    "    \"Bosque de Chapultepec I Secc, Miguel Hidalgo, 11850 Ciudad de México, CDMX, USA\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86d1fc4-a574-480c-a9fe-1bc119f12f97",
   "metadata": {},
   "source": [
    "### Gold Label Validation\n",
    "\n",
    "We need to evaluate this new method against our gold labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e2ef9f-b652-43f9-9f27-4a1b58bebea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_match_country(row: pd.Series) -> pd.Series:\n",
    "    \"\"\"parse_match Strict address matching\"\"\"\n",
    "    return parse_match_address_country(row[\"Address1\"], row[\"Address2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ad7b4b-1f33-499e-8a84-d62f32b6635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df, grouped_df = gold_label_report(gold_df, [strict_parse_match, parse_match_country])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c5d0a1-222a-4eba-8387-206ab6e30d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b8a76c-ede2-44dd-b50c-94765ee65fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_df = raw_df[raw_df[\"parse_match_country_correct\"]]\n",
    "print(f\"Total accurate matches for strict_parse_match: {len(true_df):,}\")\n",
    "\n",
    "true_df.sort_values(by=\"Description\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f1119a-c9a8-40d1-8253-856b5495d712",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_df = raw_df[raw_df[\"parse_match_country_correct\"] == False]\n",
    "print(f\"Total mismatches for strict_parse_match: {len(false_df):,}\")\n",
    "\n",
    "false_df.sort_values(by=\"Description\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7aa526-33e4-4705-a453-585ab7f3869a",
   "metadata": {},
   "source": [
    "# Machine Learning Approaches to Address Matching\n",
    "\n",
    "In this section we pursue two machine learning approaches to address matching, in order of sophistication. First we fine-tune a pre-trained embedding model to our task, try it on our data and search for a threshold similarity that results in good performance for our address matching problem. Second we build a Siamese BERT network model based on [Sentence-BERT](https://arxiv.org/abs/1908.10084) to classify pairs of addresses as match or mismatch. We will train it using the same dataset we use to fine-tune a sentence transformer, and if we have enough training data this will likely be a more powerful approach.\n",
    "\n",
    "## Text Embeddings, Sentence Encoding, `SentenceTransformers`, Vector Distance and Cosine Similarity\n",
    "\n",
    "Text embeddings are trained on large volumes of text that include addresses. As a result they have some understanding of address strings and can do a form of semantic comparison that is less explicit than logical comparisons with address parsing. They're an important benchmark to explore. Huggingface has an excellent [introduction to sentence similarity](https://huggingface.co/tasks/sentence-similarity).\n",
    "\n",
    "In our first machine learning approach, we are going to use transfer learning to load a pre-trained [sentence transformer](https://sbert.net) models from huggingface. We will use the training data we've prepared to fine-tune this model to our task, before rigorously evaluating it along with our other approaches.\n",
    "\n",
    "Sentence transformers sentence encode strings of different distances into fixed-length vectors, a technique called sentence encoding. Once two address strings are embedded into a pair of equal length vectors, they can be compared with cosine similarity to get a distance, the inverse of which is a similarity score.\n",
    "\n",
    "### Convert our `pd.DataFrame` to a `List[sentence_transformers.InputExample]`\n",
    "\n",
    "First we need to convert our Pandas `DataFrame` to a list of sentence transformer input examples. `InputExamples` require two fields `texts=List[str, str]` and `label`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b105924-4872-4abf-a10c-4d77150f1b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, tmp_df = train_test_split(augment_results_df, test_size=0.2, shuffle=True)\n",
    "eval_df, test_df = train_test_split(tmp_df, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"sentence1\": train_df[\"Address1\"].tolist(),\n",
    "    \"sentence2\": train_df[\"Address2\"].tolist(),\n",
    "    \"label\": train_df[\"Label\"].tolist(),\n",
    "})\n",
    "\n",
    "eval_dataset = Dataset.from_dict({\n",
    "    \"sentence1\": eval_df[\"Address1\"].tolist(),\n",
    "    \"sentence2\": eval_df[\"Address2\"].tolist(),\n",
    "    \"label\": eval_df[\"Label\"].tolist(),\n",
    "})\n",
    "\n",
    "test_dataset = Dataset.from_dict({\n",
    "    \"sentence1\": test_df[\"Address1\"].tolist(),\n",
    "    \"sentence2\": test_df[\"Address2\"].tolist(),\n",
    "    \"label\": test_df[\"Label\"].tolist(),\n",
    "})\n",
    "\n",
    "print(f\"Training data:   {len(train_df):,}\")\n",
    "print(f\"Validation data: {len(eval_df):,}\")\n",
    "print(f\"Test data        {len(eval_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680e51e3-84d0-40e5-97d1-ebce61cd9514",
   "metadata": {},
   "source": [
    "### Configure Fine-Tuning, Initialize a `SentenceTransformer`\n",
    "\n",
    "To use the training data we prepared to fine-tune a `SentenceTransformer`, we need to select and load a pre-trained model from Huggingface Hub. Here are some models you can try:\n",
    "\n",
    "* [sentence-transformers/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) - multilingual paraphrase models are designed to compare sentences in terms of their semantics.\n",
    "* [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3) - a robust, multilingual model optimized for a variety of tasks\n",
    "* [sentence-transformers/paraphrase-multilingual-mpnet-base-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2) - MPNet is another paraphrase model architecture we can fine-tune for address comparison\n",
    "* [sentence-transformers/all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) - a top performing MPNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a2a181-a9cd-469f-9b1b-32b9d3163dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SBERT_MODEL = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "VARIANT = \"original\"\n",
    "MODEL_SAVE_NAME = (SBERT_MODEL + \"-\" + VARIANT).replace(\"/\", \"-\")\n",
    "\n",
    "EPOCHS = 6\n",
    "BATCH_SIZE = 32\n",
    "PATIENCE = 2\n",
    "LEARNING_RATE = .00005\n",
    "DATASET_MULTIPLE = CLONES_PER_RUN * RUNS_PER_EXAMPLE\n",
    "SBERT_OUTPUT_FOLDER = f\"data/fine-tuned-sbert-{MODEL_SAVE_NAME}\"\n",
    "SAVE_EVAL_STEPS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e174e134-4761-4732-a2f7-98a5a785155c",
   "metadata": {},
   "source": [
    "### Initialize Weights & Biases\n",
    "\n",
    "Weights and biases `wandb` package makes it simple to monitor the performance of your training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68f7a53-d09a-4e8b-b5e9-f62857c12e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weights & Biases\n",
    "wandb.init(\n",
    "    entity=\"rjurney\",\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"libpostal-reborn\",\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"variant\": VARIANT,\n",
    "        \"dataset_multiple\": DATASET_MULTIPLE,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"patience\": PATIENCE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"sbert_model\": SBERT_MODEL,\n",
    "        \"sbert_output_folder\": SBERT_OUTPUT_FOLDER,\n",
    "        \"save_eval_steps\": SAVE_EVAL_STEPS,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0f82a6-1fb7-45fe-b468-9df611413f1e",
   "metadata": {},
   "source": [
    "### Setup our `SentenceTransformer` Model\n",
    "\n",
    "Choose the model to fine-tune above in `SBERT_MODEL` and instantiate it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e40fb24-77a6-4291-b0c5-076f8a9997d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = SentenceTransformer(\n",
    "    SBERT_MODEL,\n",
    "    device=device,\n",
    "    model_card_data=SentenceTransformerModelCardData(\n",
    "        language=\"en\",\n",
    "        license=\"apache-2.0\",\n",
    "        model_name=f\"{SBERT_MODEL}-address-matcher-{VARIANT}\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7285812b-dde6-4af0-b0d0-f3ed7c777a51",
   "metadata": {},
   "source": [
    "### Evaluate our Model Before Fine-Tuning\n",
    "\n",
    "Let's see what it can do without fine-tuning, then we'll compare our subjective results afterwards. This won't work very well, fine-tuning is required!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd45043-cac4-47d1-af34-c80fbac09e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sbert_compare(address1: str, address2: str) -> float:\n",
    "    \"\"\"sbert_compare - sentence encode each address into a fixed-length text embedding.\n",
    "    Fixed-length means they can be compared with cosine similarity.\"\"\"\n",
    "    embedding1 = sbert_model.encode(address1)\n",
    "    embedding2 = sbert_model.encode(address2)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    return 1 - distance.cosine(embedding1, embedding2)\n",
    "\n",
    "\n",
    "def sbert_match(row: pd.Series) -> pd.Series:\n",
    "    \"\"\"sbert_match - SentenceTransformer address matching, float iytoyt\"\"\"\n",
    "    return sbert_compare(row[\"Address1\"], row[\"Address2\"])\n",
    "\n",
    "\n",
    "def sbert_compare_binary(address1: str, address2: str, threshold: float = 0.5) -> Literal[0, 1]:\n",
    "    \"\"\"sbert_match - compare and return a binary match\"\"\"\n",
    "    similarity = sbert_compare(address1, address2)\n",
    "    return 1 if similarity >= threshold else 0\n",
    "\n",
    "\n",
    "def sbert_match_binary(row: pd.Series, threshold: float = 0.5) -> pd.Series:\n",
    "    \"\"\"sbert_match_binary - SentenceTransformer address matching, binary output\"\"\"\n",
    "    return sbert_compare_binary(row[\"Address1\"], row[\"Address2\"], threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b083ad79-6b50-414f-83de-a3ae68f44c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still too similar - very hard to train them away from this behavior!\n",
    "sbert_compare(\n",
    "    \"101 Oak Lane, Atlanta, GA 30308\",\n",
    "    \"102 Oak Lane, Atlanta, GA 30308\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65dad9f-3f08-439d-8ee8-2b14b740a55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A little bit further away ...\n",
    "sbert_compare(\n",
    "    \"101 Oak Lane, Atlanta, GA 30308\",\n",
    "    \"101 Oak Ln., Atlanta, GA 30308\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f869c21a-8b41-4520-87bd-7dbc8d37d1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Properly distant ...\n",
    "sbert_compare(\n",
    "    \"3413 Sean Way, Lawrenceville, GA 30044\",\n",
    "    \"1202 Oak Rd., Lawrenceville, GA 30304\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aba7fb-a560-4869-b05d-414a03a34542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Properly similar ...\n",
    "sbert_compare(\n",
    "    \"3413 Sean Way, Lawrenceville, GA 30044\",\n",
    "    \"3413 Sean Way, Lawrenceville, GA 30044, USA\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a0a9d8-8c06-45dd-9ff4-7b056e078e5a",
   "metadata": {},
   "source": [
    "### Evaluate the Test Set with the Untrained Model\n",
    "\n",
    "Let's see how well the [paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) model does on its own. This is our baseline score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e9a873-9044-443b-9741-3ab3fed0a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the evaluator\n",
    "binary_acc_evaluator = BinaryClassificationEvaluator(\n",
    "    sentences1=eval_dataset[\"sentence1\"],\n",
    "    sentences2=eval_dataset[\"sentence2\"],\n",
    "    labels=eval_dataset[\"label\"],\n",
    "    name=SBERT_MODEL,\n",
    ")\n",
    "binary_acc_evaluator(sbert_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d22615-249a-4ee7-b7c5-43fb1cada980",
   "metadata": {},
   "source": [
    "### Computing Metrics with `sklearn.metrics`\n",
    "\n",
    "We use [scikit-learn metrics](https://scikit-learn.org/stable/modules/model_evaluation.html) instead to compute our evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31111f8e-0167-4a6b-abff-dc3ecf391687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will rapidly train the embedding model. MultipleNegativesRankingLoss did not work.\n",
    "loss = losses.ContrastiveLoss(model=sbert_model)\n",
    "\n",
    "sbert_args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=SBERT_OUTPUT_FOLDER,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    warmup_ratio=0.1,\n",
    "    run_name=SBERT_MODEL,\n",
    "    load_best_model_at_end=True,\n",
    "    save_steps=SAVE_EVAL_STEPS,\n",
    "    eval_steps=SAVE_EVAL_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    greater_is_better=False,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    ")\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=sbert_model,\n",
    "    args=sbert_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    loss=loss,\n",
    "    evaluator=binary_acc_evaluator,\n",
    "    compute_metrics=compute_sbert_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=PATIENCE)],\n",
    ")\n",
    "\n",
    "trainer.evaluate()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0340162d-e84f-496b-bc8c-f730d327205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f52923-5fc9-4e0c-8e5d-d2cb21d68246",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ad1454-2902-483b-9500-63456cfa5e8d",
   "metadata": {},
   "source": [
    "### Try the Model from Our Best Epoch\n",
    "\n",
    "We fine-tuned the model for `EPOCHS` nubmer of epochs, but the last epoch isn't always best. The `TrainingArgument` `load_best_model_at_end=True` loads the model at the end.\n",
    "\n",
    "Another way to load the best model is to load our output folder and evaluate that `SentenceTransformer` on some examples to get a gestalt sense for its performance.\n",
    "\n",
    "```python\n",
    "sbert_model = SentenceTransformer(OUTPUT_FOLDER, device=device)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f2c9c5-5f70-4f13-8b3a-8e24d8509872",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_compare(\n",
    "    \"101 Oak Lane, Atlanta, GA 30308\",\n",
    "    \"102 Oak Lane, Atlanta, GA 30308\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246f35ba-5e5f-4801-a5dd-e0b54bc34d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_compare(\n",
    "    \"101 Oak Lane, Macon, GA 30308\",\n",
    "    \"101 Oak Lane, Atlanta, GA 30408\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96ef967-0c99-4301-9c2c-6909360566e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_compare(\n",
    "    \"101 Oak Lane, Atlanta, GA 30308\",\n",
    "    \"101 Oak Ln., Atlanta, GA 30308\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eb3771-17c6-4d85-9bad-8615c1eb8db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_compare(\n",
    "    \"3413 Sean Way, Lawrenceville, GA 30044\",\n",
    "    \"1202 Oak Rd., Lawrenceville, GA 30304\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c78fd25-be01-4052-b717-6471730d92ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_compare(\n",
    "    \"3413 Sean Way, Lawrenceville, GA 30044\",\n",
    "    \"3413 Sean Way, Lawrenceville, GA 30044, USA\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441a2b8b-2525-4c7c-98c0-6a65f69661b0",
   "metadata": {},
   "source": [
    "### Evaluate ROC Curve to Determine Optimum Similarity Threshold\n",
    "\n",
    "0.5 is an arbitrary line on which to divide positive (match, 1) and negative (mismatch, 0). Let's evaluate the ROC Curve of the F1 score to see what it should be set to. Recall that the `sbert_match` function has a `threshold: float = 0.5` argument.\n",
    "\n",
    "#### Evaluate on our Augmented Test Dataset\n",
    "\n",
    "First we'll evaluate the ROC curve on our augmented test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53242e2-7ce0-4c00-853f-277ba98b6317",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_df[\"Label\"]\n",
    "y_scores = test_df.apply(sbert_match, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb6dc8-528b-4680-9794-5da506b02502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "\n",
    "# Compute F1 score for each threshold\n",
    "f1_scores = [f1_score(y_true, y_scores >= t) for t in thresholds]\n",
    "\n",
    "# Find the threshold that maximizes the F1 score\n",
    "best_threshold_index = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_threshold_index]\n",
    "best_f1_score = f1_scores[best_threshold_index]\n",
    "\n",
    "print(f'Best Threshold: {best_threshold}')\n",
    "print(f'Best F1 Score: {best_f1_score}')\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_scores)\n",
    "print(f'AUC-ROC: {roc_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737566a7-4902-468f-b87e-585650d484fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for Seaborn\n",
    "pr_data = pd.DataFrame({\n",
    "    'Precision': precision[:-1],\n",
    "    'Recall': recall[:-1],\n",
    "    'F1 Score': f1_scores\n",
    "})\n",
    "\n",
    "# Plot Precision-Recall curve using Seaborn\n",
    "sns.lineplot(data=pr_data, x='Recall', y='Precision', marker='o')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Augmented Test Set Precision-Recall Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f50818-d35c-43b9-b022-9746ce96cdc2",
   "metadata": {},
   "source": [
    "### Plot a ROC Curve for our Gold Labeled Data\n",
    "\n",
    "We need to see the ROC Curve for our gold labeled data as well. We care more about performance on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3eae08-beef-419a-bb8a-ffe74fae4cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = gold_df[\"Label\"]\n",
    "y_scores = gold_df.apply(sbert_match, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bfde9f-9610-4b47-88d6-4432d5081f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "\n",
    "# Compute F1 score for each threshold\n",
    "f1_scores = [f1_score(y_true, y_scores >= t) for t in thresholds]\n",
    "\n",
    "# Find the threshold that maximizes the F1 score\n",
    "best_threshold_index = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_threshold_index]\n",
    "best_f1_score = f1_scores[best_threshold_index]\n",
    "\n",
    "print(f'Best Threshold: {best_threshold}')\n",
    "print(f'Best F1 Score: {best_f1_score}')\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_scores)\n",
    "print(f'AUC-ROC: {roc_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe142c8-8b4b-4384-be50-4e77413fd40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for Seaborn\n",
    "pr_data = pd.DataFrame({\n",
    "    'Precision': precision[:-1],\n",
    "    'Recall': recall[:-1],\n",
    "    'F1 Score': f1_scores\n",
    "})\n",
    "\n",
    "# Plot Precision-Recall curve using Seaborn\n",
    "sns.lineplot(data=pr_data, x='Recall', y='Precision', marker='o')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Gold Label Precision-Recall Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768d6185-8345-4460-82fc-654fd63cec04",
   "metadata": {},
   "source": [
    "### Debugging Errors on our Gold Labels\n",
    "\n",
    "Let's evaluate the data using our `gold_label_report` function with the best F1 score. Then we can view the errors and figure out where our model is failing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b16f14-ee29-4e2b-acec-61df4bfe8add",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df, grouped_df = gold_label_report(\n",
    "    gold_df,\n",
    "    [\n",
    "        # strict_parse_match,\n",
    "        # parse_match_country,\n",
    "        sbert_match_binary,\n",
    "    ],\n",
    "    threshold=best_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd151a3-f774-45ce-857f-e3d072fc803d",
   "metadata": {},
   "source": [
    "#### Label Description Group Analysis\n",
    "\n",
    "You can see the types of address pairs we are failing on. This can guide our data augmentation / programmatic labeling work at a high level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c954ca92-334d-4d73-98a2-313bbec12c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edae3d6-4085-41d9-bacb-3efde3d48e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df[\"sbert_match_binary_acc\"].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4340fd-af40-4450-bfc1-834bac0851cb",
   "metadata": {},
   "source": [
    "#### What it Got Right ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090a985a-a152-4976-8f57-890e88ef99ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truthiness analysis\n",
    "correct_df = raw_df[raw_df[\"sbert_match_binary_correct\"]].reset_index()\n",
    "print(f\"Number correct: {len(correct_df):,}\")\n",
    "\n",
    "correct_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d228d188-ed22-42ca-985b-ddd9088ac94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis\n",
    "wrong_df = raw_df[raw_df[\"sbert_match_binary_correct\"] == False].reset_index()\n",
    "print(f\"Number wrong: {len(wrong_df):,}\")\n",
    "\n",
    "wrong_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c299637d-619f-4683-b277-6c8081c4c23c",
   "metadata": {},
   "source": [
    "# Fuzzy, Structured Address Matching with Libpostal and Vector Distance\n",
    "\n",
    "There seems to be merit to both structured and embedding approaches to address matching. Let's see how an approximate approach to matching parsed address components might help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cebf8f6-2acc-49f7-8500-993b0a405b79",
   "metadata": {},
   "source": [
    "## Combining an Address Parsing and Embedding Approach\n",
    "\n",
    "Libpostal is a powerful parsing model, and it seems like parsing is a logical first step in matching addresses. Some logical combinations of fields should result in a match, while some should not. This logic can be programmed by a human, provided fuzzy matching is available at the field level. `101 Oak Lane` should match `101 Oak Ln` but not `102 Oak Lane`. `street_name` and `street_number` are separate fields in an address parsed by Libpostal. What is we got some machine learning help with field-level matching?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c4d097-22de-4477-b9ed-2e4700888bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sbert_compare(\"Oak Lane\", \"Oak Ln.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efde24e4-9963-42eb-a436-2e0964b269f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sbert_compare(\"Cheer Lane\", \"Cheer Road\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b6f3a3-5ba2-43e2-81ef-145b9f66a4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sbert_compare(\"101\", \"110\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79ab110-cc3c-4ea7-a5a3-b72dd383e723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sbert_compare(\"5th\", \"Fifth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf32bb3-b165-40d8-ab19-38829a2b1c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sbert_compare(\"USA\", \"United States\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f4a121-44b1-4441-8742-a6688c2a96b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sbert_compare(\"30044\", \"30308\")\n",
    "parse_address(\"3413 Sean Way\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0715a288-d141-474a-a5f8-f5cbb7e0bd9a",
   "metadata": {},
   "source": [
    "## Fine-Tuning a Lowercase `SentenceTransformer`\n",
    "\n",
    "My first pass at this method did not work whatsoever - the performance of the matcher was abysmal. This was because Libpostal *lowercases* addresses when it parses them, and I did NOT do that to the training data on a first pass :) Once I did that and retrained below - things worked much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeb24d9-ef8c-4f39-a709-8630c749a809",
   "metadata": {},
   "source": [
    "### Lowercase our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2c3ad6-8418-4fdc-98aa-e4187b648020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower_augment_results_df = augment_results_df.copy(deep=True)\n",
    "\n",
    "# lower_augment_results_df[\"Address1\"] = lower_augment_results_df[\"Address1\"].str.lower()\n",
    "# lower_augment_results_df[\"Address2\"] = lower_augment_results_df[\"Address2\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4cb876-ea2c-4133-a2ee-f4dbb635b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df, tmp_df = train_test_split(lower_augment_results_df, test_size=0.2, shuffle=True)\n",
    "# eval_df, test_df = train_test_split(tmp_df, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "# train_dataset = Dataset.from_dict({\n",
    "#     \"sentence1\": train_df[\"Address1\"].tolist(),\n",
    "#     \"sentence2\": train_df[\"Address2\"].tolist(),\n",
    "#     \"label\": train_df[\"Label\"].tolist(),\n",
    "# })\n",
    "\n",
    "# eval_dataset = Dataset.from_dict({\n",
    "#     \"sentence1\": eval_df[\"Address1\"].tolist(),\n",
    "#     \"sentence2\": eval_df[\"Address2\"].tolist(),\n",
    "#     \"label\": eval_df[\"Label\"].tolist(),\n",
    "# })\n",
    "\n",
    "# test_dataset = Dataset.from_dict({\n",
    "#     \"sentence1\": test_df[\"Address1\"].tolist(),\n",
    "#     \"sentence2\": test_df[\"Address2\"].tolist(),\n",
    "#     \"label\": test_df[\"Label\"].tolist(),\n",
    "# })\n",
    "\n",
    "# print(f\"Training data:   {len(train_df):,}\")\n",
    "# print(f\"Validation data: {len(eval_df):,}\")\n",
    "# print(f\"Test data        {len(eval_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0af638c-048c-47a1-98bd-bdc670d9f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SBERT_MODEL = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "# VARIANT = \"lowercase\"\n",
    "# MODEL_SAVE_NAME = (SBERT_MODEL + \"-\" + VARIANT).replace(\"/\", \"-\")\n",
    "\n",
    "# EPOCHS = 12\n",
    "# BATCH_SIZE = 32\n",
    "# PATIENCE = 2\n",
    "# LEARNING_RATE = .00005\n",
    "# DATASET_MULTIPLE = CLONES_PER_RUN * RUNS_PER_EXAMPLE\n",
    "# SBERT_OUTPUT_FOLDER = f\"data/fine-tuned-sbert-{MODEL_SAVE_NAME}\"\n",
    "# SAVE_EVAL_STEPS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450ff10b-99ce-4853-bcd2-37c17e4eb2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize Weights & Biases\n",
    "# wandb.init(\n",
    "#     entity=\"rjurney\",\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"libpostal-reborn\",\n",
    "#     # track hyperparameters and run metadata\n",
    "#     config={\n",
    "#         \"variant\": VARIANT,\n",
    "#         \"dataset_multiple\": DATASET_MULTIPLE,\n",
    "#         \"epochs\": EPOCHS,\n",
    "#         \"batch_size\": BATCH_SIZE,\n",
    "#         \"patience\": PATIENCE,\n",
    "#         \"learning_rate\": LEARNING_RATE,\n",
    "#         \"sbert_model\": SBERT_MODEL,\n",
    "#         \"sbert_output_folder\": SBERT_OUTPUT_FOLDER,\n",
    "#         \"save_eval_steps\": SAVE_EVAL_STEPS,\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc7965c-5619-44a2-a07f-9646078db1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sbert_model_lower = SentenceTransformer(\n",
    "#     SBERT_MODEL,\n",
    "#     device=device,\n",
    "#     model_card_data=SentenceTransformerModelCardData(\n",
    "#         language=\"en\",\n",
    "#         license=\"apache-2.0\",\n",
    "#         model_name=f\"{SBERT_MODEL}-address-matcher-{VARIANT}\",\n",
    "#     ),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05ff3a5-111d-4f1b-bdaa-2fc4c5859346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the evaluator\n",
    "# binary_acc_evaluator = BinaryClassificationEvaluator(\n",
    "#     sentences1=eval_dataset[\"sentence1\"],\n",
    "#     sentences2=eval_dataset[\"sentence2\"],\n",
    "#     labels=eval_dataset[\"label\"],\n",
    "#     name=SBERT_MODEL,\n",
    "# )\n",
    "# binary_acc_evaluator(sbert_model_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16800ca1-1f40-40de-9299-f8e88b0ee2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This will rapidly train the embedding model. MultipleNegativesRankingLoss did not work.\n",
    "# loss = losses.ContrastiveLoss(model=sbert_model_lower)\n",
    "\n",
    "# sbert_args = SentenceTransformerTrainingArguments(\n",
    "#     output_dir=SBERT_OUTPUT_FOLDER,\n",
    "#     num_train_epochs=EPOCHS,\n",
    "#     per_device_train_batch_size=BATCH_SIZE,\n",
    "#     per_device_eval_batch_size=BATCH_SIZE,\n",
    "#     warmup_ratio=0.1,\n",
    "#     run_name=SBERT_MODEL,\n",
    "#     load_best_model_at_end=True,\n",
    "#     save_steps=SAVE_EVAL_STEPS,\n",
    "#     eval_steps=SAVE_EVAL_STEPS,\n",
    "#     save_strategy=\"steps\",\n",
    "#     eval_strategy=\"steps\",\n",
    "#     greater_is_better=False,\n",
    "#     metric_for_best_model=\"eval_loss\",\n",
    "#     learning_rate=LEARNING_RATE,\n",
    "# )\n",
    "\n",
    "# trainer = SentenceTransformerTrainer(\n",
    "#     model=sbert_model_lower,\n",
    "#     args=sbert_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=eval_dataset,\n",
    "#     loss=loss,\n",
    "#     evaluator=binary_acc_evaluator,\n",
    "#     compute_metrics=compute_sbert_metrics,\n",
    "#     callbacks=[EarlyStoppingCallback(early_stopping_patience=PATIENCE)],\n",
    "# )\n",
    "\n",
    "# trainer.evaluate()\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0092c331-3ab7-436b-ac7e-0cf0bfd0406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6739716e-4567-47e7-a49a-87228bc33f3f",
   "metadata": {},
   "source": [
    "### Rewrite our Matchers for Lowercase Duty\n",
    "\n",
    "Need two versions of these to compare the original with their new lowercase cousins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b1761b-001e-4fd8-b61a-7d31686bc9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sbert_compare_lower(address1: str, address2: str) -> float:\n",
    "#     \"\"\"sbert_compare - sentence encode each address into a fixed-length text embedding.\n",
    "#     Fixed-length means they can be compared with cosine similarity.\"\"\"\n",
    "#     embedding1 = sbert_model_lower.encode(address1.lower())\n",
    "#     embedding2 = sbert_model_lower.encode(address2.lower())\n",
    "\n",
    "#     # Compute cosine similarity\n",
    "#     return 1 - distance.cosine(embedding1, embedding2)\n",
    "\n",
    "\n",
    "# def sbert_match_lower(row: pd.Series) -> pd.Series:\n",
    "#     \"\"\"sbert_match - SentenceTransformer address matching, float iytoyt\"\"\"\n",
    "#     return sbert_compare_lower(row[\"Address1\"], row[\"Address2\"])\n",
    "\n",
    "\n",
    "# def sbert_compare_binary_lower(address1: str, address2: str, threshold: float = 0.5) -> Literal[0, 1]:\n",
    "#     \"\"\"sbert_match - compare and return a binary match\"\"\"\n",
    "#     similarity = sbert_compare_lower(address1, address2)\n",
    "#     return 1 if similarity >= threshold else 0\n",
    "\n",
    "\n",
    "# def sbert_match_binary_lower(row: pd.Series, threshold: float = 0.5) -> pd.Series:\n",
    "#     \"\"\"sbert_match_binary - SentenceTransformer address matching, binary output\"\"\"\n",
    "#     return sbert_compare_binary_lower(row[\"Address1\"], row[\"Address2\"], threshold=threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94e3805-efb3-4677-97a6-03f469484ac0",
   "metadata": {},
   "source": [
    "### Evaluate ROC Curve to Determine Optimum Similarity Threshold\n",
    "\n",
    "We need to evaluate the ROC Curve of the F1 score to see what it should be set to for our lowercase model too. Recall that the `sbert_match_lower` function has a `threshold: float = 0.5` argument.\n",
    "\n",
    "#### Evaluate on our Augmented Test Dataset\n",
    "\n",
    "First we'll evaluate the ROC curve on our augmented test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245e39c8-4434-4e20-993e-b6dcf4b8bd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true = test_df[\"Label\"]\n",
    "# y_scores = test_df.apply(sbert_match_lower, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044d9937-5c3e-4e9a-aff2-e30aec095d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute precision-recall curve\n",
    "# precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "\n",
    "# # Compute F1 score for each threshold\n",
    "# f1_scores = [f1_score(y_true, y_scores >= t) for t in thresholds]\n",
    "\n",
    "# # Find the threshold that maximizes the F1 score\n",
    "# best_threshold_index = np.argmax(f1_scores)\n",
    "# best_threshold = thresholds[best_threshold_index]\n",
    "# best_f1_score = f1_scores[best_threshold_index]\n",
    "\n",
    "# print(f'Best Threshold: {best_threshold}')\n",
    "# print(f'Best F1 Score: {best_f1_score}')\n",
    "\n",
    "# roc_auc = roc_auc_score(y_true, y_scores)\n",
    "# print(f'AUC-ROC: {roc_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3d1d6e-8ca0-4964-aa69-91e2e7ddffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a DataFrame for Seaborn\n",
    "# pr_data = pd.DataFrame({\n",
    "#     'Precision': precision[:-1],\n",
    "#     'Recall': recall[:-1],\n",
    "#     'F1 Score': f1_scores\n",
    "# })\n",
    "\n",
    "# # Plot Precision-Recall curve using Seaborn\n",
    "# sns.lineplot(data=pr_data, x='Recall', y='Precision', marker='o')\n",
    "# plt.xlabel('Recall')\n",
    "# plt.ylabel('Precision')\n",
    "# plt.title('Augmented Test Set Precision-Recall Curve')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3a978d-6193-4968-99b3-65cfbd584c4e",
   "metadata": {},
   "source": [
    "### Plot a ROC Curve for our Gold Labeled Data\n",
    "\n",
    "We need to see the ROC Curve for our gold labeled data as well. We care more about performance on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb225e7-ad73-4236-b4ea-49fbb1da3bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true = gold_df[\"Label\"]\n",
    "# y_scores = gold_df.apply(sbert_match_lower, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d2c166-d6c2-481c-b573-f1c5ca6fd41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute precision-recall curve\n",
    "# precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "\n",
    "# # Compute F1 score for each threshold\n",
    "# f1_scores = [f1_score(y_true, y_scores >= t) for t in thresholds]\n",
    "\n",
    "# # Find the threshold that maximizes the F1 score\n",
    "# best_threshold_index = np.argmax(f1_scores)\n",
    "# best_threshold = thresholds[best_threshold_index]\n",
    "# best_f1_score = f1_scores[best_threshold_index]\n",
    "\n",
    "# print(f'Best Threshold: {best_threshold}')\n",
    "# print(f'Best F1 Score: {best_f1_score}')\n",
    "\n",
    "# roc_auc = roc_auc_score(y_true, y_scores)\n",
    "# print(f'AUC-ROC: {roc_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50070e2-5c1e-4367-b5be-d1145d87e97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a DataFrame for Seaborn\n",
    "# pr_data = pd.DataFrame({\n",
    "#     'Precision': precision[:-1],\n",
    "#     'Recall': recall[:-1],\n",
    "#     'F1 Score': f1_scores\n",
    "# })\n",
    "\n",
    "# # Plot Precision-Recall curve using Seaborn\n",
    "# sns.lineplot(data=pr_data, x='Recall', y='Precision', marker='o')\n",
    "# plt.xlabel('Recall')\n",
    "# plt.ylabel('Precision')\n",
    "# plt.title('Gold Label Precision-Recall Curve')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e133fb1b-6fcd-406a-82a0-8d9e9fac579e",
   "metadata": {},
   "source": [
    "### Rewriting our Structured Matcher\n",
    "\n",
    "Let's rewrite our original parser to use `sbert_compare_binary(address1: str, address2: str, threshold=best_threshold)` for the road name, city (which we skipped before) and country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31be0a9-8281-4ed4-92e3-753a8f040ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_fuzzy_match_address(address1: str, address2: str, threshold=0.5) -> Literal[0, 1]:\n",
    "#     \"\"\"parse_fuzzy_match_address uses parsed addresses with fuzzy matching for street, city and country.\"\"\"\n",
    "#     address1 = to_dict(parse_address(address1))\n",
    "#     address2 = to_dict(parse_address(address2))\n",
    "\n",
    "#     def match_road(address1: Dict, address2: Dict) -> Literal[0, 1]:\n",
    "#         \"\"\"match_road - literal road matching, negative if either lacks a road\"\"\"\n",
    "#         if (\"road\" in address1) and (\"road\" in address2):\n",
    "\n",
    "#             # to_dict produces a list if two tuples have the same key\n",
    "#             if isinstance(address1[\"road\"], list):\n",
    "#                 address1[\"road\"] = \" \".join(address1[\"road\"])\n",
    "#             if isinstance(address2[\"road\"], list):\n",
    "#                 address2[\"road\"] = \" \".join(address2[\"road\"])\n",
    "            \n",
    "#             if sbert_compare_binary_lower(\n",
    "#                 address1[\"road\"],\n",
    "#                 address2[\"road\"], \n",
    "#                 threshold=threshold\n",
    "#             ):\n",
    "#                 logger.debug(\"road match\")\n",
    "#                 return 1\n",
    "#             else:\n",
    "#                 logger.debug(\"road mismatch\")\n",
    "#                 return 0\n",
    "#         logger.debug(\"road mismatch\")\n",
    "#         return 0\n",
    "\n",
    "#     def match_house_number(address1: Dict, address2: Dict) -> Literal[0, 1]:\n",
    "#         \"\"\"match_house_number - literal house number matching, negative if either lacks a house_number\"\"\"\n",
    "#         if (\"house_number\" in address1) and (\"house_number\" in address2):\n",
    "#             if address1[\"house_number\"] == address2[\"house_number\"]:\n",
    "#                 logger.debug(\"house_number match\")\n",
    "#                 return 1\n",
    "#             else:\n",
    "#                 logger.debug(\"house_number mismatch\")\n",
    "#                 return 0\n",
    "#         logger.debug(\"house_number mistmatch\")\n",
    "#         return 0\n",
    "\n",
    "#     def match_unit(address1: Dict, address2: Dict) -> Literal[0, 1]:\n",
    "#         \"\"\"match_unit - note a missing unit in both is a match\"\"\"\n",
    "#         if \"unit\" in address1:\n",
    "#             if \"unit\" in address2:\n",
    "#                 logger.debug(\"unit match\")\n",
    "#                 return 1 if (address1[\"unit\"] == address2[\"unit\"]) else 0\n",
    "#             else:\n",
    "#                 logger.debug(\"unit mismatch\")\n",
    "#                 return 0\n",
    "#         if \"unit\" in address2:\n",
    "#             if \"unit\" in address1:\n",
    "#                 logger.debug(\"unit match\")\n",
    "#                 return 1 if (address1[\"unit\"] == address2[\"unit\"]) else 0\n",
    "#             else:\n",
    "#                 logger.debug(\"unit mismatch\")\n",
    "#                 return 0\n",
    "#         # Neither address has a unit, which is a default match\n",
    "#         return 1\n",
    "\n",
    "#     def match_postcode(address1: Dict, address2: Dict) -> Literal[0, 1]:\n",
    "#         \"\"\"match_postcode - literal matching, negative if either lacks a postal code\"\"\"\n",
    "#         if (\"postcode\" in address1) and (\"postcode\" in address2):\n",
    "#             if address1[\"postcode\"] == address2[\"postcode\"]:\n",
    "#                 logger.debug(\"postcode match\")\n",
    "#                 return 1\n",
    "#             else:\n",
    "#                 logger.debug(\"postcode mismatch\")\n",
    "#                 return 0\n",
    "#         logger.debug(\"postcode mismatch\")\n",
    "#         return 0\n",
    "\n",
    "#     def match_country(address1: Dict, address2: Dict) -> Literal[0, 1]:\n",
    "#         \"\"\"match_country - literal country matching - pass if both don't have one\"\"\"\n",
    "#         if (\"country\" in address1) and (\"country\" in address2):\n",
    "\n",
    "#             # to_dict produces a list if two tuples have the same key\n",
    "#             if isinstance(address1[\"country\"], list):\n",
    "#                 address1[\"country\"] = \" \".join(address1[\"country\"])\n",
    "#             if isinstance(address2[\"country\"], list):\n",
    "#                 address2[\"country\"] = \" \".join(address2[\"country\"])\n",
    "\n",
    "#             if sbert_compare_binary_lower(\n",
    "#                 address1[\"country\"],\n",
    "#                 address2[\"country\"],\n",
    "#                 threshold=best_threshold,\n",
    "#             ):\n",
    "#                 logger.debug(\"country match\")\n",
    "#                 return 1\n",
    "#             else:\n",
    "#                 logger.debug(\"country mismatch\")\n",
    "#                 return 0\n",
    "\n",
    "#         # One or none countries should match\n",
    "#         logger.debug(\"country match\")\n",
    "#         return 1\n",
    "\n",
    "#     # Combine the above to get a complete address matcher\n",
    "#     if (\n",
    "#         match_road(address1, address2)\n",
    "#         and match_house_number(address1, address2)\n",
    "#         and match_unit(address1, address2)\n",
    "#         and match_postcode(address1, address2)\n",
    "#         and match_country(address1, address2)\n",
    "#     ):\n",
    "#         logger.debug(\"overall match\")\n",
    "#         return 1\n",
    "#     else:\n",
    "#         logger.debug(\"overall mismatch\")\n",
    "#         return 0\n",
    "\n",
    "\n",
    "# def sbert_parse_match(row: pd.Series, threshold: float = 0.5) -> pd.Series:\n",
    "#     \"\"\"fuzzy_parse_match Fuzzy, structured address matching. Threshold is passed through via gold_label_report.\"\"\"\n",
    "#     return parse_fuzzy_match_address(row[\"Address1\"], row[\"Address2\"], threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394be39d-eb55-4ca5-9d56-f14d06c220ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_df, grouped_df = gold_label_report(\n",
    "#     gold_df,\n",
    "#     [\n",
    "#         strict_parse_match,\n",
    "#         parse_match_country,\n",
    "#         sbert_match_binary,\n",
    "#         sbert_parse_match,\n",
    "#     ],\n",
    "#     threshold=best_threshold\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4554ba8-9280-4b86-a5e9-7610aca02a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_df[\"sbert_parse_match_acc\"].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf346df-68a4-47e0-8e6d-ff1c237a0a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Truthiness analysis\n",
    "# correct_df = raw_df[raw_df[\"sbert_parse_match_correct\"]].reset_index(drop=True)\n",
    "# print(f\"Number correct: {len(correct_df):,}\")\n",
    "\n",
    "# correct_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a67103-0ac3-4254-872b-0c245ca4ddf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Error analysis\n",
    "# wrong_df = raw_df[raw_df[\"sbert_parse_match_correct\"] == False].reset_index()\n",
    "# print(f\"Number wrong: {len(wrong_df):,}\")\n",
    "\n",
    "# wrong_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39042e3c-7a97-4589-97d2-af269c7d11c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sbert_compare_lower(\"nw 5th ave\", \"northwest 5th avenue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb15a9f-1863-4bc3-822f-5057f8baab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_fuzzy_match_address(\"2024 NW 5th Ave, Miami, FL 33127\", \"2024 Northwest 5th Avenue, Miami, Florida 33127\", threshold=best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d839232a-9e11-4954-8968-fb7f83597ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_fuzzy_match_address(\"Third Ave, New York, NY\", \"3rd Avenue, New York, New York\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730af59f-37d8-408b-b1ea-fba55ca6f5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sbert_compare_lower(\"101 market square, seattle, wa 98039\", \"101 davis place, seattle, wa 98039\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6ea0fe-c542-4ff1-8010-16fa5e20084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_fuzzy_match_address(\"221B Baker Street, London, NW1 6XE, UK\", \"221B Baker St, Marylebone, London NW1 6XE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ce4d63-059e-4830-a0b6-4b0b9ab515f7",
   "metadata": {},
   "source": [
    "## Fuzzy Parsed Conclusion\n",
    "\n",
    "If you look at the items missed by this model, they are largely due to stricter matching requirements. These could be addressed logically, as with the help of fuzzy matching with `SentenceTransformers` they fall within the scope of task a human can accomplish. This means the structured, fuzzy model can be trusted more than the pure `SentenceTransformer` model. A little more work could bring it into a state that meets or beats the pure `SentenceTransformer` model while providing explainability that entity resolution often requires."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0388ebba-f964-41ce-ab37-c9c295a45806",
   "metadata": {},
   "source": [
    "## Structured Prediction with a `Sentence-BERT` Classifier\n",
    "\n",
    "Embeddings as a solution to this problem have a side-effect of optimizing an embedding for information retrieval... but they ignore the structure of parsed addresses. A deep network that is aware of it can perform better. Let's try out an implementation of the Sentence-BERT model, which was outlined by Nils Reimers and Iryna Gurevych in the original paper that created sentence tranformers, [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\n",
    "](https://arxiv.org/abs/1908.10084)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6f9841-6248-4e06-81b7-c5761fde354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SBERT_MODEL = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "VARIANT = \"untrained-embeddings\"\n",
    "MODEL_SAVE_NAME = (\"Sentence-BERT\" + \"-\" + VARIANT).replace(\"/\", \"-\")\n",
    "\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 8\n",
    "PATIENCE = 2\n",
    "LEARNING_RATE = 2e-5\n",
    "MODEL_OUTPUT_FOLDER = f\"data/{MODEL_SAVE_NAME}\"\n",
    "SAVE_EVAL_STEPS = 100\n",
    "\n",
    "COLUMN_SPECIAL_CHAR = \"[COL]\"\n",
    "VALUE_SPECIAL_CHAR = \"[VAL]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb214a-2cbb-48d2-9e42-307aab992516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weights & Biases\n",
    "wandb.init(\n",
    "    entity=\"rjurney\",\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"libpostal-reborn\",\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"model\": \"Sentence-BERT\",\n",
    "        \"variant\": VARIANT,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"patience\": PATIENCE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"sbert_model\": SBERT_MODEL,\n",
    "        \"model_output_folder\": MODEL_OUTPUT_FOLDER,\n",
    "        \"save_eval_steps\": SAVE_EVAL_STEPS,\n",
    "        \"model_save_name\": MODEL_SAVE_NAME,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaed279-2818-4a22-84bd-26a48fba64e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceBERT(torch.nn.Module):\n",
    "    def __init__(self, model_name=SBERT_MODEL, dim=384):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.ffnn = torch.nn.Linear(dim*3, 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_pool(token_embeds, attention_mask):\n",
    "        in_mask = attention_mask.unsqueeze(-1).expand(token_embeds.size()).float()\n",
    "        pool = torch.sum(token_embeds * in_mask, 1) / torch.clamp(in_mask.sum(1), min=1e-9)\n",
    "        return pool\n",
    "\n",
    "    def _check_similarity(self, a, b, mask_a, mask_b):\n",
    "        u = self.model(a, attention_mask=mask_a)[0]\n",
    "        v = self.model(b, attention_mask=mask_b)[0]\n",
    "        u = SentenceBERT.mean_pool(u, mask_a)\n",
    "        v = SentenceBERT.mean_pool(v, mask_b)\n",
    "        uv = torch.abs(u - v)\n",
    "        x = torch.cat([u, v, uv], dim=-1)\n",
    "        x = torch.sigmoid(self.ffnn(x).float())\n",
    "        return x\n",
    "\n",
    "    def check_similarity(self, a, b):\n",
    "        encoded_a = self.tokenizer(a, padding=True, truncation=True, return_tensors='pt')\n",
    "        encoded_b = self.tokenizer(b, padding=True, truncation=True, return_tensors='pt')\n",
    "        a = encoded_a['input_ids']\n",
    "        b = encoded_b['input_ids']\n",
    "        mask_a = encoded_a['attention_mask']\n",
    "        mask_b = encoded_b['attention_mask']\n",
    "        with torch.no_grad():\n",
    "            return self._check_similarity(a, b, mask_a, mask_b)\n",
    "\n",
    "    def forward(self, input_ids_a, input_ids_b, attention_mask_a=None, attention_mask_b=None, labels=None):\n",
    "        logits = self._check_similarity(input_ids_a, input_ids_b, attention_mask_a, attention_mask_b)\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "            labels = labels.float().unsqueeze(1)\n",
    "            loss = loss_fct(logits, labels)\n",
    "            return loss, logits\n",
    "        return logits\n",
    "\n",
    "    def predict(self, a: str, b: str):\n",
    "        with torch.no_grad():\n",
    "            logits = self.check_similarity(a, b)\n",
    "            probabilities = torch.sigmoid(logits)\n",
    "            predicted_class = (probabilities > 0.5).long().item()\n",
    "            return predicted_class, probabilities.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15494649-c8bc-495c-9a10-7516c315b6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = SentenceBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916eb7c6-fa55-4207-8e50-724dfae27189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structured_encode_address(address: List[Tuple[str, str]]) -> str:\n",
    "    \"\"\"structured_parse_address - encode a parsed address\"\"\"\n",
    "    sorted_address: List[Tuple[str, str]] = sorted(address, key=lambda x: x[0])\n",
    "    encoded_address: str = str()\n",
    "    for col, val in sorted_address:\n",
    "        encoded_address += COLUMN_SPECIAL_CHAR + col + VALUE_SPECIAL_CHAR + val\n",
    "    return encoded_address\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    encoded_a = classifier_model.tokenizer(examples[\"sentence1\"], padding=\"max_length\", truncation=True)\n",
    "    encoded_b = classifier_model.tokenizer(examples[\"sentence2\"], padding=\"max_length\", truncation=True)\n",
    "    return {\n",
    "        \"input_ids_a\": encoded_a[\"input_ids\"],\n",
    "        \"attention_mask_a\": encoded_a[\"attention_mask\"],\n",
    "        \"input_ids_b\": encoded_b[\"input_ids\"],\n",
    "        \"attention_mask_b\": encoded_b[\"attention_mask\"],\n",
    "        \"labels\": examples[\"label\"]\n",
    "    }\n",
    "\n",
    "\n",
    "def format_dataset(dataset):\n",
    "    dataset.set_format(type=\"torch\", columns=[\"input_ids_a\", \"attention_mask_a\", \"input_ids_b\", \"attention_mask_b\", \"labels\"])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac83aeb-e907-4724-9da7-c95f4cac124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, tmp_df = train_test_split(augment_results_df, test_size=0.2, shuffle=True)\n",
    "eval_df, test_df = train_test_split(tmp_df, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"sentence1\": train_df[\"Address1\"].tolist(),\n",
    "    \"sentence2\": train_df[\"Address2\"].tolist(),\n",
    "    \"label\": train_df[\"Label\"].tolist(),\n",
    "})\n",
    "\n",
    "eval_dataset = Dataset.from_dict({\n",
    "    \"sentence1\": eval_df[\"Address1\"].tolist(),\n",
    "    \"sentence2\": eval_df[\"Address2\"].tolist(),\n",
    "    \"label\": eval_df[\"Label\"].tolist(),\n",
    "})\n",
    "\n",
    "test_dataset = Dataset.from_dict({\n",
    "    \"sentence1\": test_df[\"Address1\"].tolist(),\n",
    "    \"sentence2\": test_df[\"Address2\"].tolist(),\n",
    "    \"label\": test_df[\"Label\"].tolist(),\n",
    "})\n",
    "\n",
    "print(f\"Training data:   {len(train_df):,}\")\n",
    "print(f\"Validation data: {len(eval_df):,}\")\n",
    "print(f\"Test data        {len(eval_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee830c6-cea1-4113-b398-827e5f101643",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_train_dataset = format_dataset(tokenized_train_dataset)\n",
    "tokenized_eval_dataset = format_dataset(tokenized_eval_dataset)\n",
    "tokenized_test_dataset = format_dataset(tokenized_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e650246e-27b7-4135-bbf6-8320a8858b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"data/{MODEL_SAVE_NAME}\",\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    warmup_ratio=0.1,\n",
    "    run_name=SBERT_MODEL,\n",
    "    load_best_model_at_end=True,\n",
    "    save_steps=SAVE_EVAL_STEPS,\n",
    "    eval_steps=SAVE_EVAL_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    greater_is_better=False,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        input_ids_a = inputs.pop(\"input_ids_a\")\n",
    "        input_ids_b = inputs.pop(\"input_ids_b\")\n",
    "        attention_mask_a = inputs.pop(\"attention_mask_a\")\n",
    "        attention_mask_b = inputs.pop(\"attention_mask_b\")\n",
    "        outputs = model(input_ids_a, input_ids_b, attention_mask_a, attention_mask_b, labels)\n",
    "        loss = outputs[0]\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=classifier_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    compute_metrics=compute_classifier_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=PATIENCE)],\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3d4025-ec9b-45cd-a3fd-b764ab1b9920",
   "metadata": {},
   "source": [
    "### ReWrite our Evaluation Code for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808a4f57-c875-4ec2-9bf1-4fc2a1971afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_match(row: pd.Series) -> pd.Series:\n",
    "    \"\"\"sbert_match - SentenceTransformer address matching, float iytoyt\"\"\"\n",
    "    return model.predict(row[\"Address1\"], row[\"Address2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e135a0db-3cdb-4d1e-a0ce-9ccf97ca2b19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
