{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d0f6845-e53f-4e9b-be24-0e376269f522",
   "metadata": {},
   "source": [
    "# Address Matching with a Libpostal and a Categorical Classifier\n",
    "\n",
    "This notebook matches addresses by parsing them with Libpostal and using that structure in a categorical transformers deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8feab8c8-a116-40f9-a882-5a932140af26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from numbers import Number\n",
    "from typing import Callable, Dict, List, Literal, Sequence, Tuple, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "import pytest\n",
    "import random\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from datasets import Dataset\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "from postal.parser import parse_address\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_curve,\n",
    "    precision_recall_fscore_support,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    auc\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers import InputExample, SentenceTransformer, SentencesDataset, SentenceTransformerTrainer, losses\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, SimilarityFunction, BinaryClassificationEvaluator\n",
    "from sentence_transformers.model_card import SentenceTransformerModelCardData\n",
    "from sentence_transformers.training_args import BatchSamplers, SentenceTransformerTrainingArguments\n",
    "from tenacity import retry\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import RAdam\n",
    "from tqdm.autonotebook import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel, EarlyStoppingCallback, TrainingArguments, Trainer\n",
    "from transformers.integrations import WandbCallback\n",
    "\n",
    "from utils import (\n",
    "    augment_gold_labels,\n",
    "    compute_sbert_metrics,\n",
    "    compute_classifier_metrics,\n",
    "    format_dataset,\n",
    "    gold_label_report,\n",
    "    preprocess_logits_for_metrics,\n",
    "    structured_encode_address,\n",
    "    tokenize_function,\n",
    "    to_dict,\n",
    "    save_transformer,\n",
    "    load_transformer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0578d26d-790c-4fe6-aed7-2529f155f7f6",
   "metadata": {},
   "source": [
    "#### Pin Random Seeds for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b495c03-2aba-4d6b-889b-3b21b56f4d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 31337\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.mps.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e998da-0660-4274-b000-619b28c3c533",
   "metadata": {},
   "source": [
    "#### Setup Basic Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e508b00c-0011-4845-a4d6-cd131ec5b6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stderr, level=logging.ERROR)\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2912a2be-9e56-4bf9-94fc-3dbcfd02ef17",
   "metadata": {},
   "source": [
    "#### Ignore Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18cd4a2c-f1f5-4c82-8c15-a52dc87db947",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ba8945-a02c-4661-a865-c03ee6eecebe",
   "metadata": {},
   "source": [
    "#### Configure Weights & Biases\n",
    "\n",
    "`wandb` needs some environment variables to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9491984a-c9fb-40d5-8fb0-743b77d15c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_LOG_MODEL\"] = \"end\"\n",
    "os.environ[\"WANDB_WATCH\"] = \"gradients\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"libpostal-reborn\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"false\"\n",
    "os.environ[\"WANDB_IGNORE_GLOBS\"] = \".env\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9006aa62-5aac-4bd2-900e-09b8107903e3",
   "metadata": {},
   "source": [
    "#### Optionally Disable `wandb` Uploads\n",
    "\n",
    "Weights and Biases can be slow..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17afaccb-e7db-48a5-aaff-561804821337",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_MODE\"] = \"online\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e1d411-e2a5-4824-9309-ff50861a0e19",
   "metadata": {},
   "source": [
    "#### Configure Huggingface APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62d0d44b-c38a-48ca-a3b1-895b5b8c320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_ENDPOINT\"] = \"https://huggingface.co/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7462e0-b3d1-4d76-bd7a-803cbd2f3446",
   "metadata": {},
   "source": [
    "#### Configure Huggingface APIs\n",
    "\n",
    "Squash any warnings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fcde550-79a6-4333-967e-c80d555de5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42913e46-e5db-4eff-9410-5a8449d4cbf6",
   "metadata": {},
   "source": [
    "#### Configure Pandas to Show More Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2d4cb15-b201-4f34-8917-d292dd2a1876",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 40)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad42226-1abc-4c29-b68d-659d330ba1f9",
   "metadata": {},
   "source": [
    "### Use CUDA or MPS if Avaialable\n",
    "\n",
    "CPU training and even inference with sentence transformers and deep learning models is quite slow. Since all machine learning in this library is based on [PyTorch](https://pytorch.org/get-started/locally/), we can assign all ML operations to a GPU in this one block of code. Otherwise we default to CPU without acceleration. The notebook is still workable in this mode, you just may need to grab a cup of tea or coffee while you wait for it to train the Sentence-BERT model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1051aa37-26b7-496a-8182-f10cfa1f5f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for CUDA or MPS availability and set the device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    logger.debug(\"Using Apple GPU acceleration\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    logger.debug(\"Using NVIDIA CUDA GPU acceleration\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    logger.debug(\"Using CPU for ML\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921a41eb-4e6b-4d64-a3b4-46013524e37c",
   "metadata": {},
   "source": [
    "### Use Weights & Biases for Logging Metrics\n",
    "\n",
    "Weights & Biases has a free account for individuals with public projects. Using it will produce charts during our training runs that anyone can view. You can create your own project for this notebook and login with that key to log your own training runs.\n",
    "\n",
    "You may need to run the following command from your shell before the next cell, otherwise you will have to paste your project key into the \n",
    "\n",
    "```bash\n",
    "wandb login\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d98cdf2f-fcb9-495e-9201-80f9e07d9070",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrjurney\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Login to wandb. Comment out if you already haven't via `wandb login` from a CLI\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f1a4c5-0bcd-4c52-b5d7-7be7ad1932ea",
   "metadata": {},
   "source": [
    "### Literal is Too Precise!\n",
    "\n",
    "While it is useful to parse addresses and implement literal matching logic as we did above, as the third example indicates, an abbreviation or a single typo results in a mistmatch. We're going to write a more complex, approximate logical matcher below using string distance and text embeddings.\n",
    "\n",
    "Depending on your application you might relax this criteria to include corner cases such as missing postcodes. Before we get into that, let's create some training and evaluation data using hand-labeled data with an LLM data augmentation strategy to generate a lot of labeled records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15895b85-1d0f-441c-9d20-0849b3db3b22",
   "metadata": {},
   "source": [
    "## Data Augmentation with the OpenAI GPT4o API\n",
    "\n",
    "We need training data for our supervised learning approaches to addres matching. Open the sister notebook [Address Data Augmentation.ipynb](Address%20Matching%20Deep%20Dive.ipynb) before procceeding to further cells in order to create some training data via minimal manual labeling and programmatic data labeling for data augmentation. This will teach you programmatic data labeling, a critical skill that LLMs make MUCH EASIER because they understand things like the semantics of global addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "833e8045-5cd3-4b5a-9cd3-afc65965062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df = pd.read_csv(\"data/gold.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6382fc3-ca84-4d17-9d74-354ab798d21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Address1</th>\n",
       "      <th>Address2</th>\n",
       "      <th>Description</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123 E Main St, Springfield, IL 62701</td>\n",
       "      <td>123 East Main Street, Springfield, Illinois 62701</td>\n",
       "      <td>Different directional prefix formats for same ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>456 W Elm St, Boston, MA 02118</td>\n",
       "      <td>456 West Elm Street, Boston, Massachusetts 02118</td>\n",
       "      <td>Different directional prefix formats for same ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>789 S Oak St, Denver, CO 80203</td>\n",
       "      <td>789 South Oak Street, Denver, Colorado 80203</td>\n",
       "      <td>Different directional prefix formats for same ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>321 N Pine St, Seattle, WA 98101</td>\n",
       "      <td>321 North Pine Street, Seattle, Washington 98101</td>\n",
       "      <td>Different directional prefix formats for same ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>654 E Maple St, Austin, TX 73301</td>\n",
       "      <td>654 East Maple Street, Austin, Texas 73301</td>\n",
       "      <td>Different directional prefix formats for same ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>987 W Cedar St, San Francisco, CA 94102</td>\n",
       "      <td>987 West Cedar Street, San Francisco, Californ...</td>\n",
       "      <td>Different directional prefix formats for same ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>246 S Birch St, New York, NY 10001</td>\n",
       "      <td>246 South Birch Street, New York, New York 10001</td>\n",
       "      <td>Different directional prefix formats for same ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>135 N Cedar Ave, Chicago, IL 60601</td>\n",
       "      <td>135 North Cedar Avenue, Chicago, Illinois 60601</td>\n",
       "      <td>Different directional prefix formats for same ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>864 E Pine Ave, Los Angeles, CA 90001</td>\n",
       "      <td>864 East Pine Avenue, Los Angeles, California ...</td>\n",
       "      <td>Different directional prefix formats for same ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>753 W Spruce St, Houston, TX 77001</td>\n",
       "      <td>753 West Spruce Street, Houston, Texas 77001</td>\n",
       "      <td>Different directional prefix formats for same ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>159 S Maple Ave, Phoenix, AZ 85001</td>\n",
       "      <td>159 South Maple Avenue, Phoenix, Arizona 85001</td>\n",
       "      <td>Different directional prefix formats for same ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>357 N Oak St, Philadelphia, PA 19102</td>\n",
       "      <td>357 North Oak Street, Philadelphia, Pennsylvan...</td>\n",
       "      <td>Different directional prefix formats for same ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>468 E Birch Ave, San Antonio, TX 78201</td>\n",
       "      <td>468 East Birch Avenue, San Antonio, Texas 78201</td>\n",
       "      <td>Different directional prefix formats for same ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>579 W Cedar St, Dallas, TX 75201</td>\n",
       "      <td>579 West Cedar Street, Dallas, Texas 75201</td>\n",
       "      <td>Different directional prefix formats for same ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>681 S Pine Ave, San Jose, CA 95101</td>\n",
       "      <td>681 South Pine Avenue, San Jose, California 95101</td>\n",
       "      <td>Different directional prefix formats for same ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>792 N Maple St, Jacksonville, FL 32201</td>\n",
       "      <td>792 North Maple Street, Jacksonville, Florida ...</td>\n",
       "      <td>Different directional prefix formats for same ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>903 E Oak St, Columbus, OH 43201</td>\n",
       "      <td>903 East Oak Street, Columbus, Ohio 43201</td>\n",
       "      <td>Different directional prefix formats for same ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>104 W Elm Ave, Charlotte, NC 28201</td>\n",
       "      <td>104 West Elm Avenue, Charlotte, North Carolina...</td>\n",
       "      <td>Different directional prefix formats for same ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>215 S Cedar St, Indianapolis, IN 46201</td>\n",
       "      <td>215 South Cedar Street, Indianapolis, Indiana ...</td>\n",
       "      <td>Different directional prefix formats for same ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>326 N Pine Ave, Fort Worth, TX 76101</td>\n",
       "      <td>326 North Pine Avenue, Fort Worth, Texas 76101</td>\n",
       "      <td>Different directional prefix formats for same ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Address1  \\\n",
       "0      123 E Main St, Springfield, IL 62701   \n",
       "1            456 W Elm St, Boston, MA 02118   \n",
       "2            789 S Oak St, Denver, CO 80203   \n",
       "3          321 N Pine St, Seattle, WA 98101   \n",
       "4          654 E Maple St, Austin, TX 73301   \n",
       "5   987 W Cedar St, San Francisco, CA 94102   \n",
       "6        246 S Birch St, New York, NY 10001   \n",
       "7        135 N Cedar Ave, Chicago, IL 60601   \n",
       "8     864 E Pine Ave, Los Angeles, CA 90001   \n",
       "9        753 W Spruce St, Houston, TX 77001   \n",
       "10       159 S Maple Ave, Phoenix, AZ 85001   \n",
       "11     357 N Oak St, Philadelphia, PA 19102   \n",
       "12   468 E Birch Ave, San Antonio, TX 78201   \n",
       "13         579 W Cedar St, Dallas, TX 75201   \n",
       "14       681 S Pine Ave, San Jose, CA 95101   \n",
       "15   792 N Maple St, Jacksonville, FL 32201   \n",
       "16         903 E Oak St, Columbus, OH 43201   \n",
       "17       104 W Elm Ave, Charlotte, NC 28201   \n",
       "18   215 S Cedar St, Indianapolis, IN 46201   \n",
       "19     326 N Pine Ave, Fort Worth, TX 76101   \n",
       "\n",
       "                                             Address2  \\\n",
       "0   123 East Main Street, Springfield, Illinois 62701   \n",
       "1    456 West Elm Street, Boston, Massachusetts 02118   \n",
       "2        789 South Oak Street, Denver, Colorado 80203   \n",
       "3    321 North Pine Street, Seattle, Washington 98101   \n",
       "4          654 East Maple Street, Austin, Texas 73301   \n",
       "5   987 West Cedar Street, San Francisco, Californ...   \n",
       "6    246 South Birch Street, New York, New York 10001   \n",
       "7     135 North Cedar Avenue, Chicago, Illinois 60601   \n",
       "8   864 East Pine Avenue, Los Angeles, California ...   \n",
       "9        753 West Spruce Street, Houston, Texas 77001   \n",
       "10     159 South Maple Avenue, Phoenix, Arizona 85001   \n",
       "11  357 North Oak Street, Philadelphia, Pennsylvan...   \n",
       "12    468 East Birch Avenue, San Antonio, Texas 78201   \n",
       "13         579 West Cedar Street, Dallas, Texas 75201   \n",
       "14  681 South Pine Avenue, San Jose, California 95101   \n",
       "15  792 North Maple Street, Jacksonville, Florida ...   \n",
       "16          903 East Oak Street, Columbus, Ohio 43201   \n",
       "17  104 West Elm Avenue, Charlotte, North Carolina...   \n",
       "18  215 South Cedar Street, Indianapolis, Indiana ...   \n",
       "19     326 North Pine Avenue, Fort Worth, Texas 76101   \n",
       "\n",
       "                                          Description  Label  \n",
       "0   Different directional prefix formats for same ...    1.0  \n",
       "1   Different directional prefix formats for same ...    1.0  \n",
       "2   Different directional prefix formats for same ...    1.0  \n",
       "3   Different directional prefix formats for same ...    1.0  \n",
       "4   Different directional prefix formats for same ...    1.0  \n",
       "5   Different directional prefix formats for same ...    1.0  \n",
       "6   Different directional prefix formats for same ...    1.0  \n",
       "7   Different directional prefix formats for same ...    1.0  \n",
       "8   Different directional prefix formats for same ...    1.0  \n",
       "9   Different directional prefix formats for same ...    1.0  \n",
       "10  Different directional prefix formats for same ...    1.0  \n",
       "11  Different directional prefix formats for same ...    1.0  \n",
       "12  Different directional prefix formats for same ...    1.0  \n",
       "13  Different directional prefix formats for same ...    1.0  \n",
       "14  Different directional prefix formats for same ...    1.0  \n",
       "15  Different directional prefix formats for same ...    1.0  \n",
       "16  Different directional prefix formats for same ...    1.0  \n",
       "17  Different directional prefix formats for same ...    1.0  \n",
       "18  Different directional prefix formats for same ...    1.0  \n",
       "19  Different directional prefix formats for same ...    1.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want to start from here and not run the data augmentation pipeline again...\n",
    "augment_results_df = pd.read_parquet(\"data/training.6.parquet\")\n",
    "\n",
    "augment_results_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2b2e4e-0c28-47ce-8b3c-7c1dac0062f5",
   "metadata": {},
   "source": [
    "### Data Augmentation Complete!\n",
    "\n",
    "Starting by hand labeling under 100 records and iterating a few times on data augmentation instructions for GPT4o, we have multiplied them by many times to get almost 10,000 synthetic records! This is enough to fine-tune a `SentenceTransformer` or semantic text similarity classifier model. GPT4o is a powerful tool for data augmentation! This can work for a variety of problems.\n",
    "\n",
    "LLM based data augmentation is a powerful tool for your data labeling toolbox."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423992c9-bd0c-4e85-b5ce-cbf14a1d5356",
   "metadata": {},
   "source": [
    "# Comparing Different Approaches to Address Matching\n",
    "\n",
    "Now we're going to compare the following methods of address matching:\n",
    "\n",
    "1) Database Lookups - we'l use [pycountry](https://pypi.org/project/pycountry/) ([github](https://github.com/pycountry/pycountry)) to improve international address matching (see [PyCountry Nation Matching](PyCountry%20Nation%20Matching.ipynb)).\n",
    "2) Text Embeddings - we'll use transfer learning to load an existing [SentenceTransformer](https://sbert.net) model to sentence encode pairs of addresses to create fixed-length embeddings for each address and then compute a similarity score via [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity). This won't work without fine-tuning, so we fine-tune the model to the task.\n",
    "3) Deep Matching Model - We'll train a deep semantic textual similarity classification model based on a Siamese BERT network as defined in [Sentence-BERT](https://arxiv.org/abs/1908.10084) to classify address pairs as matching or not matching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7aa526-33e4-4705-a453-585ab7f3869a",
   "metadata": {},
   "source": [
    "# Machine Learning Approaches to Address Matching\n",
    "\n",
    "In this section we pursue two machine learning approaches to address matching, in order of sophistication. First we fine-tune a pre-trained embedding model to our task, try it on our data and search for a threshold similarity that results in good performance for our address matching problem. Second we build a Siamese BERT network model based on [Sentence-BERT](https://arxiv.org/abs/1908.10084) to classify pairs of addresses as match or mismatch. We will train it using the same dataset we use to fine-tune a sentence transformer, and if we have enough training data this will likely be a more powerful approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67998e4-9729-4831-993d-aca757f9aa8c",
   "metadata": {},
   "source": [
    "# Parsed Address Matching with Libpostal, PyTorch and `Cosine-Sentence-BERT`\n",
    "\n",
    "Our next strategy will be to parse the addresses using Libpostal and then to encode them in a way that perserves the parsed staructure. We will use a deep network architecture called `Cosine-Sentence-BERT`, a derivative of `Sentence-BERT`, to build a classifier for pairs of addresses that can achieve better performance than fine-tuned sentence transformers and cosine similarity alone.\n",
    "\n",
    "Embeddings as a solution to this problem have a side-effect of optimizing an embedding for information retrieval... but they ignore the structure of parsed addresses. A deep network that is aware of it can perform better. Let's try out an implementation of the Sentence-BERT model, which was outlined by Nils Reimers and Iryna Gurevych in the original paper that created sentence tranformers, [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\n",
    "](https://arxiv.org/abs/1908.10084)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02a8f5bc-c48d-4fe0-8bb6-91dac3db3609",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "def parse_standardize_address(address: str) -> str:\n",
    "    \"\"\"Parse with Libpostal, then stringify fields in standard order\"\"\"\n",
    "\n",
    "    # Libpostal parse the address\n",
    "    parsed_address: List[Tuple[str, str]] = parse_address(address)\n",
    "    \n",
    "    \n",
    "    FIELD_ORDER = [\n",
    "        \"house_number\",\n",
    "        \"house\",\n",
    "        \"road\",\n",
    "        \"unit\",\n",
    "        \"level\",\n",
    "        \"staircase\",\n",
    "        \"entrance\",\n",
    "        \"category\",\n",
    "        \"near\",\n",
    "        \"suburb\",\n",
    "        \"city_district\",\n",
    "        \"city\",\n",
    "        \"island\",\n",
    "        \"state_district\",\n",
    "        \"state\",\n",
    "        \"postcode\",\n",
    "        \"po_box\",\n",
    "        \"country_region\",\n",
    "        \"country\",\n",
    "        \"world_region\"\n",
    "    ]\n",
    "    \n",
    "    # Fields that typically precede a comma in addresses\n",
    "    COMMA_AFTER = {\"road\", \"city\", \"state\", \"country_region\"}\n",
    "    \n",
    "    # Create a defaultdict to group values by field\n",
    "    address_dict = defaultdict(list)\n",
    "    for value, field in parsed_address:\n",
    "        if value.strip():\n",
    "            address_dict[field].append(value.strip())\n",
    "    \n",
    "    # Create a list of non-empty address components in the specified order\n",
    "    address_components = []\n",
    "    for field in FIELD_ORDER:\n",
    "        if field in address_dict:\n",
    "            component = ' '.join(address_dict[field])\n",
    "            if field in COMMA_AFTER and field != FIELD_ORDER[-1]:\n",
    "                component += ','\n",
    "            address_components.append(component)\n",
    "    \n",
    "    # Manually join the components with a space unless they are null\n",
    "    address_string = str()\n",
    "    for address_component in address_components:\n",
    "        if address_component and isinstance(address_component, str) and len(address_component) > 0 and address_component != \",\":\n",
    "            address_string += address_component + \" \"\n",
    "    \n",
    "    return address_string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75976b16-ca9d-4005-9a51-e76a975f876e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:   8,024\n",
      "Validation data: 1,003\n",
      "Test data        1,003\n"
     ]
    }
   ],
   "source": [
    "train_df, tmp_df = train_test_split(augment_results_df, test_size=0.2, shuffle=True)\n",
    "eval_df, test_df = train_test_split(tmp_df, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "# Encode the addresses using [COL] / [VAL] special characters\n",
    "parsed_train_dataset = Dataset.from_dict({\n",
    "    \"sentence1\": [parse_standardize_address(x) for x in train_df[\"Address1\"].tolist()],\n",
    "    \"sentence2\": [parse_standardize_address(x) for x in train_df[\"Address2\"].tolist()],\n",
    "    \"label\": train_df[\"Label\"].tolist(),\n",
    "})\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"sentence1\": train_df[\"Address1\"].tolist(),\n",
    "    \"sentence2\": train_df[\"Address2\"].tolist(),\n",
    "    \"label\": train_df[\"Label\"].tolist(),\n",
    "})\n",
    "\n",
    "# Encode the addresses using [COL] / [VAL] special characters\n",
    "parsed_eval_dataset = Dataset.from_dict({\n",
    "    \"sentence1\": [parse_standardize_address(x) for x in eval_df[\"Address1\"].tolist()],\n",
    "    \"sentence2\": [parse_standardize_address(x) for x in eval_df[\"Address2\"].tolist()],\n",
    "    \"label\": eval_df[\"Label\"].tolist(),\n",
    "})\n",
    "eval_dataset = Dataset.from_dict({\n",
    "    \"sentence1\": eval_df[\"Address1\"].tolist(),\n",
    "    \"sentence2\": eval_df[\"Address2\"].tolist(),\n",
    "    \"label\": eval_df[\"Label\"].tolist(),\n",
    "})\n",
    "\n",
    "# Encode the addresses using [COL] / [VAL] special characters\n",
    "parsed_test_dataset = Dataset.from_dict({\n",
    "    \"sentence1\": [parse_standardize_address(x) for x in test_df[\"Address1\"].tolist()],\n",
    "    \"sentence2\": [parse_standardize_address(x) for x in test_df[\"Address2\"].tolist()],\n",
    "    \"label\": test_df[\"Label\"].tolist(),\n",
    "})\n",
    "test_dataset = Dataset.from_dict({\n",
    "    \"sentence1\": test_df[\"Address1\"].tolist(),\n",
    "    \"sentence2\": test_df[\"Address2\"].tolist(),\n",
    "    \"label\": test_df[\"Label\"].tolist(),\n",
    "})\n",
    "\n",
    "print(f\"Training data:   {len(train_df):,}\")\n",
    "print(f\"Validation data: {len(eval_df):,}\")\n",
    "print(f\"Test data        {len(eval_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc6f9841-6248-4e06-81b7-c5761fde354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SBERT_MODEL = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "VARIANT = \"raw-embeddings\"\n",
    "MODEL_SAVE_NAME = (\"CategoricalAddressMatcher\" + \"-\" + VARIANT).replace(\"/\", \"-\")\n",
    "\n",
    "EPOCHS = 6\n",
    "BATCH_SIZE = 16\n",
    "PATIENCE = 2\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 0.02\n",
    "MODEL_OUTPUT_FOLDER = f\"data/{MODEL_SAVE_NAME}\"\n",
    "SAVE_EVAL_STEPS = 100\n",
    "\n",
    "LIBPOSTAL = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bdbe34-9f5c-4d82-9f56-37612435439f",
   "metadata": {},
   "source": [
    "### Substitute Libpostal Parsed `Datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9628f1e6-122e-4ea9-a084-afaba2f0a91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Using LIBPOSTAL dataset\n"
     ]
    }
   ],
   "source": [
    "if LIBPOSTAL:\n",
    "    logging.error(\"Using LIBPOSTAL dataset\")\n",
    "    train_dataset = parsed_train_dataset\n",
    "    eval_dataset = parsed_eval_dataset\n",
    "    test_dataset = parsed_test_dataset\n",
    "else:\n",
    "    logging.error(\"Using raw dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dfb214a-2cbb-48d2-9e42-307aab992516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rjurney/Software/libpostal-reborn/wandb/run-20240630_213413-imypgsnb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rjurney/libpostal-reborn/runs/imypgsnb' target=\"_blank\">fiery-puddle-137</a></strong> to <a href='https://wandb.ai/rjurney/libpostal-reborn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rjurney/libpostal-reborn' target=\"_blank\">https://wandb.ai/rjurney/libpostal-reborn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rjurney/libpostal-reborn/runs/imypgsnb' target=\"_blank\">https://wandb.ai/rjurney/libpostal-reborn/runs/imypgsnb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/rjurney/libpostal-reborn/runs/imypgsnb?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f610c733910>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Weights & Biases\n",
    "wandb.init(\n",
    "    entity=\"rjurney\",\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"libpostal-reborn\",\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"model\": \"CategoricalAddressMatcher\",\n",
    "        \"variant\": VARIANT,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"patience\": PATIENCE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"sbert_model\": SBERT_MODEL,\n",
    "        \"model_output_folder\": MODEL_OUTPUT_FOLDER,\n",
    "        \"save_eval_steps\": SAVE_EVAL_STEPS,\n",
    "        \"model_save_name\": MODEL_SAVE_NAME,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "        \"libpostal\": LIBPOSTAL,\n",
    "    },\n",
    "    save_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b7740e6-4a1b-4d5e-99bc-4eeb9d3f21c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalAddressMatcher(nn.Module):\n",
    "    def __init__(self, model_name: str = SBERT_MODEL, dim: int = 384, num_categories: int = 2):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.num_categories = num_categories\n",
    "        \n",
    "        # Freeze BERT layers\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.field_embedders = nn.ModuleDict({\n",
    "            field: nn.Linear(self.bert.config.hidden_size, dim)\n",
    "            for field in [\"house_number\", \"road\", \"city\", \"state\", \"postcode\", \"unit\", \"house\", \"suburb\", \"city_district\", \"country\"]\n",
    "        })\n",
    "        \n",
    "        self.field_attention = nn.MultiheadAttention(embed_dim=dim, num_heads=4)\n",
    "        \n",
    "        self.combine_fields = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(3 * dim, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(dim, num_categories)\n",
    "        )\n",
    "\n",
    "    def embed_field(self, field: str, value: str) -> torch.Tensor:\n",
    "        inputs = self.tokenizer(value, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        outputs = self.bert(**inputs)\n",
    "        pooled = outputs.last_hidden_state.mean(dim=1)  # Mean pooling\n",
    "        return self.field_embedders[field](pooled)\n",
    "\n",
    "    def forward_one(self, address: Dict[str, str]) -> torch.Tensor:\n",
    "        field_embeddings = []\n",
    "        for field, value in address.items():\n",
    "            if field in self.field_embedders and value:\n",
    "                embedding = self.embed_field(field, value)\n",
    "                field_embeddings.append(embedding)\n",
    "        \n",
    "        if not field_embeddings:\n",
    "            return torch.zeros(1, self.field_embedders[\"road\"].out_features)\n",
    "        \n",
    "        field_embeddings = torch.cat(field_embeddings, dim=0)\n",
    "        attended_embeddings, _ = self.field_attention(field_embeddings, field_embeddings, field_embeddings)\n",
    "        combined = self.combine_fields(attended_embeddings.mean(dim=0, keepdim=True))\n",
    "        return combined\n",
    "\n",
    "    def forward(self, address1: Dict[str, str], address2: Dict[str, str]) -> torch.Tensor:\n",
    "        embed1 = self.forward_one(address1)\n",
    "        embed2 = self.forward_one(address2)\n",
    "        \n",
    "        diff = torch.abs(embed1 - embed2)\n",
    "        concat = torch.cat([embed1, embed2, diff], dim=1)\n",
    "        \n",
    "        logits = self.classifier(concat)\n",
    "        return logits\n",
    "\n",
    "    def get_embedding(self, address: Dict[str, str]) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            return self.forward_one(address)\n",
    "\n",
    "def train_categorical_address_matcher(model: CategoricalAddressMatcher, train_data: List[Dict[str, Dict[str, str]]], labels: List[int], num_epochs: int = 5, lr: float = 2e-5):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for (addr1, addr2), label in zip(train_data, labels):\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(addr1, addr2)\n",
    "            loss = criterion(logits, torch.tensor([label], dtype=torch.long))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_data)}\")\n",
    "\n",
    "def predict_category(model: CategoricalAddressMatcher, addr1: Dict[str, str], addr2: Dict[str, str]) -> int:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(addr1, addr2)\n",
    "        return torch.argmax(logits, dim=1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d77c9e7-cee9-4821-8e2c-f7b30a3f0c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = CategoricalAddressMatcher()\n",
    "tokenizer = classifier_model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "238b0455-144a-4924-bffa-d4a01f6fe99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11931c9dc10d4499bc9e480e064a1027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8024 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ce4f3d7d5c41fe82a12f3e9dde752d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7de10b68f14a27b7e462487e2a8cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
    "\n",
    "tokenized_train_dataset = format_dataset(tokenized_train_dataset)\n",
    "tokenized_eval_dataset = format_dataset(tokenized_eval_dataset)\n",
    "tokenized_test_dataset = format_dataset(tokenized_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "171c0469-a059-4932-8b03-52cd94d8cd93",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 46\u001b[0m\n\u001b[1;32m     34\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m RAdam(classifier_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mtraining_args\u001b[38;5;241m.\u001b[39mlearning_rate)\n\u001b[1;32m     36\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CategoricalTrainer(\n\u001b[1;32m     37\u001b[0m     model\u001b[38;5;241m=\u001b[39mclassifier_model,\n\u001b[1;32m     38\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     optimizers\u001b[38;5;241m=\u001b[39m(optimizer, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m     44\u001b[0m )\n\u001b[0;32m---> 46\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/libpostal/lib/python3.11/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/libpostal/lib/python3.11/site-packages/transformers/trainer.py:2216\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/libpostal/lib/python3.11/site-packages/transformers/trainer.py:3231\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3213\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3214\u001b[0m \u001b[38;5;124;03mPerform a training step on a batch of inputs.\u001b[39;00m\n\u001b[1;32m   3215\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3228\u001b[0m \u001b[38;5;124;03m    `torch.Tensor`: The tensor with training loss on this batch.\u001b[39;00m\n\u001b[1;32m   3229\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3230\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m-> 3231\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   3234\u001b[0m     loss_mb \u001b[38;5;241m=\u001b[39m smp_forward_backward(model, inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps)\n",
      "File \u001b[0;32m~/anaconda3/envs/libpostal/lib/python3.11/site-packages/transformers/trainer.py:3184\u001b[0m, in \u001b[0;36mTrainer._prepare_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3179\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3180\u001b[0m \u001b[38;5;124;03mPrepare `inputs` before feeding them to the model, converting them to tensors if they are not already and\u001b[39;00m\n\u001b[1;32m   3181\u001b[0m \u001b[38;5;124;03mhandling potential state.\u001b[39;00m\n\u001b[1;32m   3182\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3183\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs)\n\u001b[0;32m-> 3184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   3185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3186\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe batch received was empty, your model won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be able to train on it. Double-check that your \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3187\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining dataset contains keys expected by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signature_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3188\u001b[0m     )\n\u001b[1;32m   3189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_past \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "class CategoricalTrainer(Trainer):\n",
    "    \"\"\"Trainer for Cosine-Sentence-BERT. Uses RAdam optimizer and custom loss function.\"\"\"\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs[\"loss\"]\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_OUTPUT_FOLDER,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    run_name=MODEL_SAVE_NAME,\n",
    "    load_best_model_at_end=True,\n",
    "    save_steps=SAVE_EVAL_STEPS,\n",
    "    eval_steps=SAVE_EVAL_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    greater_is_better=False,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    optim=\"adamw_torch\",\n",
    "    fp16=True if device.type == \"cuda\" else False,\n",
    "    data_seed=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "optimizer = RAdam(classifier_model.parameters(), lr=training_args.learning_rate)\n",
    "\n",
    "trainer = CategoricalTrainer(\n",
    "    model=classifier_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    compute_metrics=compute_classifier_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=PATIENCE)],\n",
    "    optimizers=(optimizer, None),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ce2275-6f76-4454-863a-1a3b8adcd829",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best model checkpoint path: {trainer.state.best_model_checkpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693e3cc0-8844-43af-a738-cc25f7778ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "pd.DataFrame([trainer.evaluate()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a393084f-aa67-48ad-ad08-93aaea71fb3b",
   "metadata": {},
   "source": [
    "### Save the Best Model\n",
    "\n",
    "Because we used `load_best_model_at_end=True`, our model is now the best one we fine-tuned. Save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70abb3b9-a9e3-4280-a659-728aeec6c52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_transformer(classifier_model, \"data/classifier_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c22cd20c-61be-47c4-9eba-eac14a1cc23b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.629 MB of 0.629 MB uploaded (0.038 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 3.2%             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">easy-cosmos-136</strong> at: <a href='https://wandb.ai/rjurney/libpostal-reborn/runs/wc8p1v5c' target=\"_blank\">https://wandb.ai/rjurney/libpostal-reborn/runs/wc8p1v5c</a><br/> View project at: <a href='https://wandb.ai/rjurney/libpostal-reborn' target=\"_blank\">https://wandb.ai/rjurney/libpostal-reborn</a><br/>Synced 6 W&B file(s), 0 media file(s), 6 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240630_213316-wc8p1v5c/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abab52e-f77c-4fe9-8c99-9bc8d77adcb9",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8112c70-6bbe-4ee1-bc0f-cacc51aa472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = load_transformer(CosineSentenceBERT, \"data/classifier_model\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b6b68f-c85f-4753-ace8-0a74e3471d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.predict(\"3413 Sean Way, Lawrenceville, GA 30044\", \"3413 Sean Way, Lawrenceville, GA 30044\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387a0065-5412-48e3-b5cb-d146aa4cd528",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.predict(\"101 Oak Ct.,\", \"101 Oak Street\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4265009-bf3e-467c-84fd-27d202cf5c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.predict(\"101 Oak Pl.\", \"101 Oak Place\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3d4025-ec9b-45cd-a3fd-b764ab1b9920",
   "metadata": {},
   "source": [
    "### Probability and Boolean Prediction Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808a4f57-c875-4ec2-9bf1-4fc2a1971afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_match(row: pd.Series) -> pd.Series:\n",
    "    \"\"\"classifier_match - Sentence-BERT address matching, float output\"\"\"\n",
    "    return classifier_model.predict(row[\"Address1\"], row[\"Address2\"])\n",
    "\n",
    "\n",
    "def classifier_match_boolean(row: pd.Series, threshold=0.5) -> pd.Series:\n",
    "    \"\"\"classifier_match_binary - Sentence-BERT address matching, boolean output\"\"\"\n",
    "    return 1 if classifier_model.predict(row[\"Address1\"], row[\"Address2\"]) > threshold else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5f79c9-a93e-4d5c-ae1f-bc819e89847a",
   "metadata": {},
   "source": [
    "### Synthetic Data Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6d6951-b186-40f0-913b-2e209b376439",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_df[\"Label\"]\n",
    "y_scores = test_df.apply(classifier_match, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d13decc-0983-4dac-936e-1c38c9e4eadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "\n",
    "# Compute F1 score for each threshold\n",
    "f1_scores = [f1_score(y_true, y_scores >= t) for t in thresholds]\n",
    "\n",
    "# Find the threshold that maximizes the F1 score\n",
    "best_threshold_index = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_threshold_index]\n",
    "best_f1_score = f1_scores[best_threshold_index]\n",
    "\n",
    "print(f'Best Threshold: {best_threshold}')\n",
    "print(f'Best F1 Score: {best_f1_score}')\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_scores)\n",
    "print(f'AUC-ROC: {roc_auc}')\n",
    "\n",
    "# Create a DataFrame for Seaborn\n",
    "pr_data = pd.DataFrame({\n",
    "    'Precision': precision[:-1],\n",
    "    'Recall': recall[:-1],\n",
    "    'F1 Score': f1_scores\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b571184-2b27-4a40-9969-02fec11c4e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall curve using Seaborn\n",
    "sns.lineplot(data=pr_data, x='Recall', y='Precision', marker='o')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Augmented Test Set Precision-Recall Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f55d107-561c-4bf7-afe4-f78172c16ae2",
   "metadata": {},
   "source": [
    "### Gold Label Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b262d8-346d-4d14-8947-7c3f61302c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = gold_df[\"Label\"]\n",
    "y_scores = gold_df.apply(classifier_match, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af49aa98-4a18-4389-bf00-ee6d10948dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "\n",
    "# Compute F1 score for each threshold\n",
    "f1_scores = [f1_score(y_true, y_scores >= t) for t in thresholds]\n",
    "\n",
    "# Find the threshold that maximizes the F1 score\n",
    "best_threshold_index = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_threshold_index]\n",
    "best_f1_score = f1_scores[best_threshold_index]\n",
    "\n",
    "print(f'Best Threshold: {best_threshold}')\n",
    "print(f'Best F1 Score: {best_f1_score}')\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_scores)\n",
    "print(f'AUC-ROC: {roc_auc}')\n",
    "\n",
    "# Create a DataFrame for Seaborn\n",
    "pr_data = pd.DataFrame({\n",
    "    'Precision': precision[:-1],\n",
    "    'Recall': recall[:-1],\n",
    "    'F1 Score': f1_scores\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a31a400-8e76-4e34-9384-82c5cd2a68ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall curve using Seaborn\n",
    "sns.lineplot(data=pr_data, x='Recall', y='Precision', marker='o')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Gold Label Precision-Recall Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e135a0db-3cdb-4d1e-a0ce-9ccf97ca2b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df, grouped_df = gold_label_report(\n",
    "    gold_df,\n",
    "    [\n",
    "        # sbert_match_binary,\n",
    "        classifier_match_boolean,\n",
    "    ],\n",
    "    threshold=best_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087f4277-693a-44d1-b7b2-81895d7b4100",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cde90e-b2db-401a-8a7b-0dd3e24edb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truthiness analysis\n",
    "correct_df = raw_df[raw_df[\"classifier_match_boolean_correct\"]].reset_index(drop=True)\n",
    "print(f\"Number correct: {len(correct_df):,}\")\n",
    "\n",
    "correct_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ed830e-88ff-4c92-addb-ea73f60a5edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis\n",
    "wrong_df = raw_df[raw_df[\"classifier_match_boolean_correct\"] == False].reset_index()\n",
    "print(f\"Number wrong: {len(wrong_df):,}\")\n",
    "\n",
    "wrong_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54205060-4522-4bea-840a-fc0aede13d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.predict(\n",
    "    \"101 Oak Lane, Atlanta, GA 30308\",\n",
    "    \"102 Oak Lane, Atlanta, GA 30308\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37beb453-3084-4e3f-b635-c4f9882f12e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.predict(\n",
    "    \"101 Oak Lane, Macon, GA 30308\",\n",
    "    \"101 Oak Lane, Atlanta, GA 30408\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29452c9c-abb1-49b7-819e-b0a42ede9440",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.predict(\n",
    "    \"101 Oak Lane, Atlanta, GA 30308\",\n",
    "    \"101 Oak Ln., Atlanta, GA 30308\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d97d1c0-4e93-47a2-b52d-7c5be5ce555d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.predict(\n",
    "    \"3413 Sean Way, Lawrenceville, GA 30044\",\n",
    "    \"1202 Oak Rd., Lawrenceville, GA 30304\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4972bc19-ba6b-4e0b-80cc-80baf013ce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.predict(\n",
    "    \"3413 Sean Way, Lawrenceville, GA 30044\",\n",
    "    \"3413 Sean Way, Lawrenceville, GA 30044, USA\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3a3f91-9a58-4a7b-986c-9d27d23dd98a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
